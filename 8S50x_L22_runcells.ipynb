{"cells": [{"cell_type": "markdown", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 22: Markov Chain Monte Carlo - Part I</h1>\n", "\n"]}, {"cell_type": "markdown", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='section_23_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L22.0 Overview</h2>\n"]}, {"cell_type": "markdown", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_23_1\">L22.1 Markov Chain MC</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_23_1\">L22.1 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_23_2\">L22.2 Understanding some details</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_23_2\">L22.2 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_23_3\">L22.3 A more realistic Markov Chain MC</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_23_3\">L22.3 Exercises</a></td>\n", "    </tr>\n", "</table>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["py", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.0-runcell00\n", "\n", "!git init\n", "!git remote add -f origin https://github.com/mitx-8s50/nb_LEARNER/\n", "!git config core.sparseCheckout true\n", "!echo 'data/L23' >> .git/info/sparse-checkout\n", "!git pull origin main\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["py", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.0-runcell01\n", "\n", "!pip install corner\n", "!pip install lmfit\n", "!pip install bilby\n", "!pip install gwpy lalsuite"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["py", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.0-runcell01\n", "\n", "import imageio\n", "from PIL import Image\n", "\n", "import lmfit\n", "import numpy as np\n", "import torch\n", "import torch.nn as nn\n", "import matplotlib.pyplot as plt\n", "import csv\n", "import math\n", "from scipy import optimize as opt\n", "from scipy import stats\n", "import matplotlib.cm as cm\n", "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n", "import corner\n", "\n", "import bilby\n", "import scipy.signal as sig\n", "from bilby.gw.source import lal_binary_black_hole\n", "from bilby.gw.conversion import convert_to_lal_binary_black_hole_parameters"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["py", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.0-runcell02\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title"]}, {"cell_type": "markdown", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='section_23_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L22.1 Markov Chain Monte Carlo </h2>  \n", "\n", "| [Top](#section_23_0) | [Previous Section](#section_23_0) | [Exercises](#exercises_23_1) | [Next Section](#section_23_2) |\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["py", "lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.1-runcell01\n", "\n", "nsample=1000\n", "values=np.random.normal(50,20,nsample)\n", "fig = plt.figure(figsize=(10,10))\n", "plt.hist(values,bins=50)\n", "plt.xlabel(\"x\")\n", "plt.ylabel(\"N\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.1-runcell02\n", "\n", "def log_like_normal(x,data):\n", "    #x[0]=mu, x[1]=sigma (new or current)\n", "    #data = the observation\n", "    return np.sum(np.log(stats.norm(x[0],x[1]).pdf(data)))\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.1-runcell03\n", "\n", "def proposal(x):\n", "    #x[0] = mu, x[1]=sigma (new or current)\n", "    x_new = np.array([0,0])\n", "    x_new = np.random.uniform(x-x[1]*np.ones(x.shape),x+x[1]*np.ones(x.shape))\n", "    return x_new\n", "\n", "#Let's run a quick test\n", "xinit=np.array([25,10])\n", "likeOld=log_like_normal(xinit,values)\n", "xtry=proposal(xinit)\n", "likeNew=log_like_normal(xtry,values)\n", "print(\"Init:\",xinit, \"Likelihood:\",likeOld)\n", "print(\"Try:\" ,xtry , \"Likelihood:\",likeNew)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.1-runcell04\n", "\n", "#note, the notation below is different than what is presented in the corresponding video\n", "\n", "\n", "#Defines whether to accept or reject the new sample\n", "def acceptance(likeOld, likeNew):\n", "    if likeNew>likeOld:\n", "        return True\n", "    else:\n", "        accept=np.random.uniform(0,1)\n", "        return (accept < (np.exp(likeNew-likeOld)))\n", "\n", "def prior(like,x):\n", "    #Adjust the likelihood by the prior\n", "    prior=1\n", "    if(x[1] <=0):\n", "        prior=0\n", "    return like+np.log(prior) #log(1)=0 so nothing gets added, log(0)=-infinity so this case always gets rejected\n", "\n", "print(\"Init Prior:\",prior(likeOld,xinit),\"Try Prior:\",prior(likeNew,xtry))\n", "print(\"Accept:\",acceptance(prior(likeOld,xinit),prior(likeNew,xtry)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.1-runcell05\n", "\n", "def metropolis_hastings(iLikelihood,iPrior,iProposal,iAcceptance,xinit,data,niterations):\n", "    x = xinit\n", "    accepted = []\n", "    rejected = []\n", "    likelihood = []\n", "    for i in range(niterations):\n", "        x_new   =  iProposal(x)  \n", "        likeOld = iLikelihood(x,data)\n", "        likeNew = iLikelihood(x_new,data) \n", "        if (iAcceptance(iPrior(likeOld,x),iPrior(likeNew,x_new))):            \n", "            x = x_new\n", "            accepted.append(x_new)\n", "            likelihood.append(likeNew)\n", "        else:\n", "            rejected.append(x_new)\n", "    accepted = np.array(accepted)\n", "    rejected = np.array(rejected)\n", "    likelihood = np.array(likelihood)\n", "    return accepted, rejected, likelihood\n", "\n", "xinit = np.array([25,10])\n", "accepted, rejected, likelihood = metropolis_hastings(log_like_normal,prior,proposal,acceptance,xinit,values,15000)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.1-runcell06\n", "\n", "#PLOT SIGMA VALUES\n", "fig = plt.figure(figsize=(10,10))\n", "ax1 = fig.add_subplot(3,1,1)\n", "print(rejected[0:50][0])\n", "\n", "ax1.plot(rejected[0:50,1], 'rx', label='Rejected sigma',alpha=0.5)\n", "ax1.plot(accepted[0:50,1], 'b.', label='Accepted sigma',alpha=0.5)\n", "ax1.set_xlabel(\"Iteration\")\n", "ax1.set_ylabel(\"$\\sigma$\")\n", "ax1.grid()\n", "ax1.legend()\n", "\n", "\n", "ax2 = fig.add_subplot(3,1,2)\n", "#to_show=-accepted.shape[0]\n", "ax2.plot( rejected[:,1], 'rx', label='Rejected sigma',alpha=0.5)\n", "ax2.plot( accepted[:,1], 'b.', label='Accepted sigma',alpha=0.5)\n", "ax2.set_xlabel(\"Iteration\")\n", "ax2.set_ylabel(\"$\\sigma$\")\n", "ax2.grid()\n", "ax2.legend()\n", "\n", "ax3 = fig.add_subplot(3,1,3)\n", "ax3.plot( likelihood, 'rx', label='Likelihood',alpha=0.5)\n", "ax3.set_xlabel(\"Iteration\")\n", "ax3.set_ylabel(\"$\\mathcal{L}$\")\n", "\n", "fig.tight_layout()\n", "accepted.shape\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.1-runcell07\n", "\n", "#PLOT MU VALUES\n", "fig = plt.figure(figsize=(10,10))\n", "ax1 = fig.add_subplot(3,1,1)\n", "print(rejected[0:50][0])\n", "\n", "ax1.plot(rejected[0:50,0], 'rx', label='Rejected mu',alpha=0.5)\n", "ax1.plot(accepted[0:50,0], 'b.', label='Accepted mu',alpha=0.5)\n", "ax1.set_xlabel(\"Iteration\")\n", "ax1.set_ylabel(\"$\\mu$\")\n", "ax1.grid()\n", "ax1.legend()\n", "\n", "\n", "ax2 = fig.add_subplot(3,1,2)\n", "ax2.plot( rejected[:,0], 'rx', label='Rejected mu',alpha=0.5)\n", "ax2.plot( accepted[:,0], 'b.', label='Accepted mu',alpha=0.5)\n", "ax2.set_xlabel(\"Iteration\")\n", "ax2.set_ylabel(\"$\\mu$\")\n", "ax2.grid()\n", "ax2.legend()\n", "\n", "ax3 = fig.add_subplot(3,1,3)\n", "ax3.plot( likelihood, 'rx', label='Likelihood',alpha=0.5)\n", "ax3.set_xlabel(\"Iteration\")\n", "ax3.set_ylabel(\"$\\mathcal{L}$\")\n", "\n", "fig.tight_layout()\n", "accepted.shape\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.1-runcell07\n", "\n", "_,bins,_ = plt.hist(rejected[:,1],bins=35,density=True,label='reject',alpha=0.5)\n", "_,bins,_ = plt.hist(accepted[:,1],bins=bins,density=True,label='accept',alpha=0.5)\n", "_,bins,_ = plt.hist(accepted[-20:,1],bins=bins,density=True,label='accept(last 20)',alpha=0.5)\n", "plt.xlabel(\"$\\sigma$\")\n", "plt.ylabel(\"pdf\")\n", "plt.legend()\n", "plt.show()\n", "\n", "_,bins,_ = plt.hist(rejected[:,0],bins=35,density=True,label='reject',alpha=0.5)\n", "_,bins,_ = plt.hist(accepted[:,0],bins=bins,density=True,label='accept',alpha=0.5)\n", "_,bins,_ = plt.hist(accepted[-20:,0],bins=bins,density=True,label='accept(last 20)',alpha=0.5)\n", "plt.xlabel(\"$\\mu$\")\n", "plt.ylabel(\"pdf\")\n", "plt.legend()\n", "plt.show()\n", "\n", "print(\"Average and stdev of all accepted mu:\",accepted[:,0].mean(),\"+/-\",accepted[:,0].std())\n", "print(\"Average and stdev of all accepted sigma:\",accepted[:,1].mean(),\"+/-\",accepted[:,1].std())\n", "\n", "print(\"Average and stdev of last 20 accepted mu:\",accepted[-20:,0].mean(),\"+/-\",accepted[-10:,0].std())\n", "print(\"Average and stdev of last 20 accepted sigma:\",accepted[-20:,1].mean(),\"+/-\",accepted[-10:,1].std())\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.1-runcell09\n", "\n", "##Analytic\n", "print(\"Analytic results for mean and standard deviation of the data:\")\n", "print(\"Mu:\",values.mean(),\"+/-\",values.std()/np.sqrt(len(values)))\n", "print(\"Sigma:\",values.std(),\"+/-\",values.std()/np.sqrt(2.*len(values)))\n", "print()\n", "\n", "y,bin_edges=np.histogram(values,bins=35)\n", "bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n", "#lmfit\n", "from lmfit.models import GaussianModel\n", "model = GaussianModel()\n", "params = model.make_params(center=25, amplitude=1, sigma=10)\n", "result = model.fit(y, params, x=bin_centers,weights=1./np.sqrt(y+1))\n", "result.plot()\n", "print(\"Fitted results for mean (\\\"center\\\") and standard deviation (\\\"sigma\\\") of the data:\")\n", "print(result.fit_report())"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.1-runcell10\n", "\n", "def proposal(x,n=1000):\n", "    #x[0] = mu, x[1]=sigma (new or current)\n", "    x_new     = x.copy()\n", "    x_new[0]  = x[0] + np.random.randn()*x[1]/np.sqrt(n)\n", "    x_new[1]  = x[1] + np.random.randn()*x[1]/np.sqrt(2*n)\n", "    return x_new\n", "\n", "xinit = np.array([25.,10.])\n", "accepted, rejected,likelihood = metropolis_hastings(log_like_normal,prior,proposal,acceptance,xinit,values,15000)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.1-runcell11\n", "\n", "plt.plot(likelihood)\n", "plt.xlabel(\"Iteration\")\n", "plt.ylabel(\"Likelihood\")\n", "plt.ylim(-4500,-4300)\n", "plt.show()\n", "\n", "_,bins,_ = plt.hist(rejected[:,1],bins=35,density=True,label='reject',alpha=0.5)\n", "_,bins,_ = plt.hist(accepted[:,1],bins=bins,density=True,label='accept',alpha=0.5)\n", "_,bins,_ = plt.hist(accepted[-20:,1],bins=bins,density=True,label='accept(last 20)',alpha=0.5)\n", "plt.xlabel(\"$\\sigma$\")\n", "plt.ylabel(\"pdf\")\n", "plt.legend()\n", "plt.show()\n", "\n", "_,bins,_ = plt.hist(rejected[:,0],bins=35,density=True,label='reject',alpha=0.5)\n", "_,bins,_ = plt.hist(accepted[:,0],bins=bins,density=True,label='accept',alpha=0.5)\n", "_,bins,_ = plt.hist(accepted[-20:,0],bins=bins,density=True,label='accept(last 20)',alpha=0.5)\n", "plt.xlabel(\"$\\mu$\")\n", "plt.ylabel(\"pdf\")\n", "plt.legend()\n", "plt.show()\n", "\n", "print(\"Average and stdev of all accepted mu:\",accepted[:,0].mean(),\"+/-\",accepted[:,0].std())\n", "print(\"Average and stdev of all accepted sigma:\",accepted[:,1].mean(),\"+/-\",accepted[:,1].std())\n", "\n", "print(\"Average and stdev of last 1000 accepted mu:\",accepted[-1000:,0].mean(),\"+/-\",accepted[-1000:,0].std())\n", "print(\"Average and stdev of last 1000 accepted sigma:\",accepted[-1000:,1].mean(),\"+/-\",accepted[-1000:,1].std())\n", "\n", "\n", "fig = plt.figure(figsize=(10,10))\n", "ax = fig.add_subplot(2,1,1)\n", "ax.plot(rejected[0:150,1], 'rx', label='Rejected',alpha=0.5)\n", "ax.plot(accepted[0:150,1], 'b.', label='Accepted',alpha=0.5)\n", "ax.set_xlabel(\"Iteration\")\n", "ax.set_ylabel(\"$\\sigma$\")\n", "ax.grid()\n", "ax.legend()\n", "\n", "\n", "ax2 = fig.add_subplot(2,1,2)\n", "#to_show=-accepted.shape[0]\n", "ax2.plot( rejected[:,1], 'rx', label='Rejected',alpha=0.5)\n", "ax2.plot( accepted[:,1], 'b.', label='Accepted',alpha=0.5)\n", "ax2.set_xlabel(\"Iteration\")\n", "ax2.set_ylabel(\"$\\sigma$\")\n", "ax2.grid()\n", "ax2.legend()\n", "\n", "\n", "\n", "fig = plt.figure(figsize=(10,10))\n", "ax = fig.add_subplot(2,1,1)\n", "ax.plot(rejected[0:150,0], 'rx', label='Rejected',alpha=0.5)\n", "ax.plot(accepted[0:150,0], 'b.', label='Accepted',alpha=0.5)\n", "ax.set_xlabel(\"Iteration\")\n", "ax.set_ylabel(\"$\\mu$\")\n", "ax.grid()\n", "ax.legend()\n", "\n", "\n", "ax2 = fig.add_subplot(2,1,2)\n", "#to_show=-accepted.shape[0]\n", "ax2.plot( rejected[:,0], 'rx', label='Rejected',alpha=0.5)\n", "ax2.plot( accepted[:,0], 'b.', label='Accepted',alpha=0.5)\n", "ax2.set_xlabel(\"Iteration\")\n", "ax2.set_ylabel(\"$\\mu$\")\n", "ax2.grid()\n", "ax2.legend()\n", "\n"]}, {"cell_type": "markdown", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='exercises_23_1'></a>     \n", "\n", "| [Top](#section_23_0) | [Restart Section](#section_23_1) | [Next Section](#section_23_2) |\n"]}, {"cell_type": "markdown", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-22.1.1</span>\n", "\n", "What happens when your proposal function gets too narrow or too wide? Edit the function `proposal_1` in the starting code shown below to try the fit with a range for finding $\\vec{\\theta^{\\prime}}$ (called `x_new` in the code) that is a factor of 10 smaller than that used in code cell `L22.1-runcell09`, and edit `proposal_2` to try with a range 10X larger. Let's focus specifically on the uncertainties in the final values of the parameters as calculated using the standard deviation of the accepted parameter sets near the end of the iterations. What changes do you observe? You could also plot the distributions of accepted and rejected points vs. iteration, to gain further insight (as done above).\n", "\n", "Try several runs to check whether or not you see a clear trend.\n", "\n", "A) The uncertainties that are computed do not change at all.\\\n", "B) The uncertainties change from run to run, but do not seem to have a consistent dependence on the proposal that is used.\\\n", "C) The uncertainties depend on the proposal that is used, and seem to be positively correlated, meaning that the computed uncertainty increases when a wider proposal function is used.\\\n", "D) The uncertainties depend on the proposal that is used, and seem to be negatively correlated (anticorrelated), meaning that the computed uncertainty decreases when a wider proposal function is used.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L22.1.1\n", "\n", "def proposal_1(x,n=2000):\n", "    #YOUR CODE HERE\n", "    return x_new\n", "\n", "xinit = np.array([25.,10.])\n", "accepted_1, rejected_1, _ = metropolis_hastings(log_like_normal,prior,proposal_1,acceptance,xinit,values,5000)\n", "\n", "print(\"1/10 original uncertainty\")\n", "print(\"Length of last 1000 accepted:\",len(accepted_1[-1000:,1]))\n", "print(\"Average and stdev of last 1000 accepted mu:\",accepted_1[-1000:,0].mean(),\"+/-\",accepted_1[-1000:,0].std())\n", "print(\"Average and stdev of last 1000 accepted sigma:\",accepted_1[-1000:,1].mean(),\"+/-\",accepted_1[-1000:,1].std())\n", "print()\n", "\n", "def proposal_2(x,n=2000):\n", "    #YOUR CODE HERE\n", "    return x_new\n", "\n", "xinit = np.array([25.,10.])\n", "accepted_2, rejected_2, _ = metropolis_hastings(log_like_normal,prior,proposal_2,acceptance,xinit,values,5000)\n", "\n", "\n", "print(\"10X original uncertainty\")\n", "print(\"Length of last 1000 accepted:\",len(accepted_2[-1000:,1]))\n", "print(\"Average and stdev of last 1000 accepted mu:\",accepted_2[-1000:,0].mean(),\"+/-\",accepted_2[-1000:,0].std())\n", "print(\"(Average and stdev of last 1000 accepted sigma:\",accepted_2[-1000:,1].mean(),\"+/-\",accepted_2[-1000:,1].std())\n", "print()\n", "\n", "\n", "#PLOT REJECTED VS. ACCEPTED\n", "#YOUR CODE HERE (if you want)"]}, {"cell_type": "markdown", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-22.1.2</span>\n", "\n", "Let's now see what happens when we use a smaller number of events to calculate the parameters and uncertainties. Using your definitions for `proposal_1` and `proposal_2` that you had above, change the final number of events you sample to take just the last 100 events? Play with the results, which answer gives you the correct result (note the cramer-rao bound holds).\n", "\n", "A) The uncertainties on both are fine.\\\n", "B) Taking the larger range is good for a wide proposal, and the smaller range is good for a narrow proposal. \\\n", "C) Taking the larger range is good for a narrow proposal, and the smaller range is good for a wide proposal. \\\n", "D) A smaller range is needed for the wide proposal because of turn on, while the narrow proposal gives uncertainties that are too small (below the Cramer-Rao bound) because it is sampling parameters with too small a proposal distribution. \n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L22.1.2\n", "\n", "def proposal_1(x,n=2000):\n", "    #YOUR CODE HERE\n", "    return x_new\n", "\n", "xinit = np.array([25.,10.])\n", "accepted_1, rejected_1, _ = metropolis_hastings(log_like_normal,prior,proposal_1,acceptance,xinit,values,5000)\n", "\n", "print(\"1/10 original uncertainty\")\n", "print(\"Length of last 100 accepted:\",len(accepted_1[-###:,1]))\n", "print(\"Average and stdev of last 100 accepted mu:\",accepted_1[-###:,0].mean(),\"+/-\",accepted_1[-###:,0].std())\n", "print(\"Average and stdev of last 100 accepted sigma:\",accepted_1[-###:,1].mean(),\"+/-\",accepted_1[-###:,1].std())\n", "print()\n", "\n", "def proposal_2(x,n=2000):\n", "    #YOUR CODE HERE\n", "    return x_new\n", "\n", "xinit = np.array([25.,10.])\n", "accepted_2, rejected_2, _ = metropolis_hastings(log_like_normal,prior,proposal_2,acceptance,xinit,values,5000)\n", "\n", "print(\"10X original uncertainty\")\n", "print(\"Length of last 100 accepted:\",len(accepted_2[-###:,1]))\n", "print(\"Average and stdev of last 100 accepted mu:\",accepted_2[-###:,0].mean(),\"+/-\",accepted_2[-###:,0].std())\n", "print(\"(Average and stdev of last 100 accepted sigma:\",accepted_2[-###,1].mean(),\"+/-\",accepted_2[-###:,1].std()))\n", "                   \n", "                                                                  \n", "#PLOT REJECTED VS. ACCEPTED\n", "#YOUR CODE HERE (if you want)"]}, {"cell_type": "markdown", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='section_23_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L22.2 Understanding Some Details </h2>  \n", "\n", "| [Top](#section_23_0) | [Previous Section](#section_23_1) | [Exercises](#exercises_23_2) | [Next Section](#section_23_3) |"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "tags": ["lect_02", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.2-runcell01\n", "\n", "#This cell plots results from our original run, which produced the array `accepted`\n", "\n", "fig = plt.figure(figsize=(10,20))\n", "ax = fig.add_subplot(3,1,1)\n", "ax.plot(accepted[:50,0], accepted[:50,1], label=\"Path\")\n", "ax.plot(accepted[:50,0], accepted[:50,1], 'b.', label='Accepted(First 50)')\n", "ax.plot(rejected[:50,0], rejected[:50,1], 'rx', label='Rejected(First 50)')\n", "ax.set_xlabel(\"$\\mu$\")\n", "ax.set_ylabel(\"$\\sigma$\")\n", "ax.legend()\n", "\n", "\n", "ax = fig.add_subplot(3,1,2)\n", "ax.plot(accepted[:,0], accepted[:,1], label=\"Path\")\n", "ax.plot(accepted[:,0], accepted[:,1], 'b.', label='Accepted(Full)',alpha=0.3)\n", "ax.plot(rejected[:,0], rejected[:,1], 'rx', label='Rejected(Full)',alpha=0.3)\n", "ax.set_xlabel(\"$\\mu$\")\n", "ax.set_ylabel(\"$\\sigma$\")\n", "ax.legend()\n", "ax.set_title(\"\") \n", "\n", "to_show=50\n", "ax = fig.add_subplot(3,1,3)\n", "ax.plot(accepted[-to_show:,0], accepted[-to_show:,1], label=\"Path\")\n", "ax.plot(accepted[-to_show:,0], accepted[-to_show:,1], 'b.', label='Accepted(Final 50)',alpha=0.5)\n", "ax.plot(rejected[-to_show:,0], rejected[-to_show:,1], 'rx', label='Rejected(Final 50)',alpha=0.5)\n", "ax.set_xlabel(\"$\\mu$\")\n", "ax.set_ylabel(\"$\\sigma$\")\n", "ax.legend()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["lect_02", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.2-runcell02\n", "\n", "mean_acc_mu=accepted[-100:,0].mean()\n", "mean_acc_sig=accepted[-100:,1].mean()\n", "print(mean_acc_mu,mean_acc_sig)\n", "\n", "def autocorr(accepted,lag):\n", "    num_mu=0\n", "    denom_mu=0\n", "    num_sig=0\n", "    denom_sig=0\n", "    rk_mu,rk_sig = 0, 0\n", "    for i in range(accepted.shape[0]-lag):\n", "        num_mu+=(accepted[i,0]-mean_acc_mu)*(accepted[i+lag,0]-mean_acc_mu)\n", "        num_sig+=(accepted[i,1]-mean_acc_sig)*(accepted[i+lag,1]-mean_acc_sig)\n", "        denom_mu+=(mean_acc_mu-accepted[i,0])**2\n", "        denom_sig+=(mean_acc_sig-accepted[i,1])**2\n", "    if denom_mu > 0 and denom_sig > 0:\n", "        rk_mu=num_mu/denom_mu\n", "        rk_sig=num_sig/denom_sig\n", "    return rk_mu, rk_sig\n", "\n", "\n", "lag=np.arange(1,100)\n", "result=np.zeros((2,lag.shape[0]))\n", "for l in lag:\n", "    result[:,l-1]=autocorr(accepted,l)\n", "\n", "\n", "fig, ax = plt.subplots()\n", "ax.plot(lag, result[1,:], label='Autocorrelation for $\\sigma$')\n", "ax.plot(lag, result[0,:], label='Autocorrelation for $\\mu$')\n", "ax.legend(loc=0)\n", "ax.set(xlabel='steps', ylabel='autocorrelation', ylim=(-1, 1))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["lect_02", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.2-runcell03\n", " \n", "labels = ['$\\mu$','$\\sigma$']\n", "print(accepted.shape)\n", "samples=accepted[-5000:-1,:]\n", "#samples=np.reshape(samples,(samples.shape[0]*samples.shape[1],samples.shape[2]))\n", "#print(samples.shape)\n", "fig = corner.corner(samples,show_titles=True,labels=labels,plot_datapoints=True,quantiles=[0.16, 0.5, 0.84])\n"]}, {"cell_type": "markdown", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 22.2.1</span>\n", "\n", "Plot the corner plot for the first 100 steps of the fit. What do your parameters look like, and do you observe a correlation (why or why not)?\n", "\n", "A) The fitted parameters are similar to those in `L22.2-runcell03`, and can be used. There is no correlation. \\\n", "B) The equilibration stage is present in the corner plots, and you see parameters move to the best fit with a positive correltion. \\\n", "C) There is a negative correlation between the parameters.\n", "<br>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L22.2.1\n", "\n", "samples=#YOUR CODE HERE\n", "fig = corner.corner(samples,show_titles=True,labels=labels,plot_datapoints=True,quantiles=[0.16, 0.5, 0.84])"]}, {"cell_type": "markdown", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='section_23_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L22.3 A More Realistic Markov Chain MC</h2>  \n", "\n", "| [Top](#section_23_0) | [Previous Section](#section_23_2) | [Exercises](#exercises_23_3) |"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["lect_03", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.3-runcell01\n", "\n", "import csv \n", "\n", "def load(iFile='data/L23/ice_core_data.txt'):\n", "    times=np.array([])\n", "    amps =np.array([])\n", "    with open(iFile, newline='') as csvfile:\n", "        line = csv.reader(csvfile, delimiter='\\t')\n", "        for row in line:\n", "            arr=row[0].split()\n", "            #print(', '.join(row))\n", "            pT=float(arr[2])\n", "            pA=float(arr[4])\n", "            times=np.append(pT,times)\n", "            amps =np.append(pA,amps)\n", "    #amps  = (amps-amps.mean())/(2.*np.max(amps))\n", "    return times,amps\n", "\n", "times,amps=load()\n", "plt.plot(times,amps)\n", "plt.xlabel('time (years)')\n", "plt.ylabel('temp (C)')\n", "plt.show()\n", "\n", "data = np.vstack((times,amps))\n", "data = data.T"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["lect_03", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.3-runcell02\n", "\n", "def func(x,t):\n", "    return x[0]*np.sin(x[1]*t) +  x[2]*np.sin(x[3]*t) +  x[4]*np.sin(x[5]*t) + x[6]\n", "\n", "def log_like(x,data):\n", "    return -1.*np.sum((func(x,data[:,0])-data[:,1])**2)\n", "\n", "def prior(like,x):\n", "    #Adjust the likelihood by the prior\n", "    prior=0\n", "    if(x[1] < x[3]) or (x[3] < x[5]) or (x[1] < x[5]):\n", "        prior=1\n", "    return like+np.log(prior)\n", "\n", "def proposal(x,n=5000):\n", "    #x[0] = mu, x[1]=sigma (new or current)\n", "    x_new     = x.copy()\n", "    #x_new[0]  = x[0] + np.random.randn()*x[0]/np.sqrt(n)\n", "    #x_new[1]  = x[1] + np.random.randn()*x[1]/np.sqrt(n)\n", "    #x_new[2]  = x[2] + np.random.randn()*x[0]/np.sqrt(n)\n", "    #x_new[3]  = x[3] + np.random.randn()*x[1]/np.sqrt(n)\n", "    #x_new[4]  = x[4] + np.random.randn()*x[0]/np.sqrt(n)\n", "    #x_new[5]  = x[5] + np.random.randn()*x[1]/np.sqrt(n)\n", "    #x_new[5]  = x[6] + np.random.randn()*x[1]/np.sqrt(n)\n", "    x_new      = x + np.random.normal(x.shape[0])*(x*0.0001+1e-6)\n", "    #np.random.normal(loc=0,scale=1,size=x.shape[0])*(x+1.0*np.ones(x.shape[0]))\n", "    return x_new\n", "\n", "def metropolis_hastings(iLikelihood,iPrior,iProposal,iAcceptance,xinit,data,niterations):\n", "    x = xinit\n", "    accepted = []\n", "    rejected = []\n", "    likelihood = []\n", "    for i in range(niterations):\n", "        x_new   =  iProposal(x)  \n", "        likeOld = iLikelihood(x,data)\n", "        likeNew = iLikelihood(x_new,data) \n", "        if (iAcceptance(iPrior(likeOld,x),iPrior(likeNew,x_new))):            \n", "            x = x_new\n", "            accepted.append(x_new)\n", "            likelihood.append(likeNew)\n", "        else:\n", "            rejected.append(x_new)\n", "    accepted = np.array(accepted)\n", "    rejected = np.array(rejected)\n", "    likelihood = np.array(likelihood)\n", "    return accepted, rejected, likelihood\n", "\n", "xinit = np.array([5.,1./10000.,5.,1./5000.,-5.,1./1000.,-5.])\n", "accepted, rejected, likelihood = metropolis_hastings(log_like,prior,proposal,acceptance,xinit,data,15000)\n", "print(accepted)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "tags": ["lect_03", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.3-runcell03\n", "\n", "plt.plot(likelihood)\n", "plt.xlabel(\"iteration\")\n", "plt.ylabel(\"$\\mathcal{L}$\")\n", "plt.show()\n", "\n", "#print(accepted[-1])\n", "plt.plot(times[0:400],amps[0:400])\n", "plt.plot(times[0:400],func(xinit,times)[0:400])\n", "plt.xlabel('time (years)')\n", "plt.ylabel('temp (C)')\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["lect_03", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.3-runcell04\n", "\n", "def proposal(x,n=5000):\n", "    return x + np.random.normal(x.shape[0])*1e-3\n", "\n", "def prior(like,x):\n", "    #Adjust the likelihood by the prior\n", "    prior=0\n", "    if(x[1] < x[3]) or (x[3] < x[5]) or (x[1] < x[5]):\n", "        prior=1\n", "    return like+np.log(prior)\n", "    \n", "def func(x,t):\n", "    return x[0]*np.sin(x[1]/10000.*t) +  x[2]*np.sin(x[3]/10000*t) +  x[4]*np.sin(x[5]/10000*t) + x[6]\n", "\n", "xinit = np.array([5.,1.,5.,2.,-5.,10.,-5.])\n", "accepted, rejected, likelihood = metropolis_hastings(log_like,prior,proposal,acceptance,xinit,data,150000)\n", "print(len(accepted))\n", "\n", "plt.plot(likelihood)\n", "plt.xlabel(\"iteration\")\n", "plt.ylabel(\"$\\mathcal{L}$\")\n", "plt.show()\n", "\n", "plt.plot(times[0:400],amps[0:400])\n", "plt.plot(times[0:400],func(xinit,times)[0:400])\n", "plt.xlabel('time (years)')\n", "plt.ylabel('temp (C)')\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["lect_03", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.3-runcell05\n", "\n", "def mc_update(walkers, otherwalkers, logp0, Npars, idata, ifunc, a=2.0,iPrint=False):\n", "    Nw = len(walkers)\n", "    No = len(otherwalkers)\n", "\n", "    # Calculates random gradient for the affine linear transformation \n", "    Z = (((a - 1.0) * np.random.rand(Nw) + 1.0) ** 2.0) / a\n", "    if iPrint:\n", "        print(\"Rand:\",Z)\n", "    \n", "    # gets random indices of the othe rwalkers\n", "    rint = np.random.randint(No, size=(Nw,))\n", "    if iPrint:\n", "        print(\"Rand Indx:\",rint)\n", "    \n", "    # Propose new positions from the linear transformation\n", "    qt1 = otherwalkers[rint] - Z[:, np.newaxis] * (otherwalkers[rint] - walkers)\n", "    if iPrint:\n", "        print(\"Rand Updates:\", qt1)\n", "    \n", "    # Calculate the posterior probability of the new position\n", "    logp1 = log_like(qt1, idata,ifunc)\n", "    #logp1 = prior(logp1,qt1)\n", "    \n", "    #Now do the usual Markov update\n", "    #p_diff = (Npars - 1) * np.log(Z) + logp - logp0\n", "    p_diff = logp1 - logp0\n", "\n", "    rshape=np.random.rand(p_diff.shape[0])\n", "    # Determine if the new positions are accepted\n", "    accept = p_diff > np.log(np.random.rand(p_diff.shape[0]))\n", "    return qt1, logp1, accept\n", "\n", "#now let's solve a 1 parameter problem\n", "#try to predict a gaussian centered at 10\n", "def tmpfunc(x,t):\n", "    out=[]\n", "    for pX in x:\n", "        val=pX*np.ones(t.shape)\n", "        out.append(val)\n", "    out = np.array(out)\n", "    return out\n", "\n", "def log_like(x,data,ifunc):\n", "    delta = ifunc(x,data[:,0])-data[:,1]*np.ones(x.shape)\n", "    return -1.*np.sum(delta**2,axis=1)\n", "\n", "Nwalkers=10\n", "Npars=1\n", "toydata  = np.random.normal(10,2,(10,2))\n", "allWalks = np.array([1 + np.random.randn(Npars) for i in range(Nwalkers)])\n", "logp0    = log_like(allWalks, toydata,tmpfunc)\n", "\n", "print(\"Inital Walks:\\n\",allWalks,\"\\nInitial p-vals:\\n\",logp0)\n", "print(\"Update first 5:\")\n", "newWalks, newlogp, accept = mc_update(allWalks[0:5], allWalks[5:10], logp0[0:5], Npars, toydata, tmpfunc,iPrint=True)\n", "print(\"Updated Walks:\\n\",newWalks,\"\\nUpdated p-vals:\\n\",newlogp)\n", "\n", "print(\"Update second 5:\")\n", "newWalks, newlogp, accept = mc_update(allWalks[5:10], allWalks[0:5], logp0[5:10], Npars, toydata, tmpfunc)\n", "print(\"Updated Walks:\\n\",newWalks,\"\\nUpdated p-vals:\\n\",newlogp)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["lect_03", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.3-runcell06\n", "\n", "def proposal(q, logp, data, Nwalk, Npars, iFunc):\n", "    half = int(Nwalk / 2)\n", "    first, second = slice(half), slice(half, Nwalk)\n", "\n", "    # Alternate slices of the data fixing and updating\n", "    for S0, S1 in [(first, second), (second, first)]:\n", "        # Use stretch move to calculate the proposal\n", "        q_new, logp_new, acc = mc_update(q[S0], q[S1], logp[S0], Npars, data, iFunc)\n", "\n", "        # Add accepted values into the chains\n", "        if np.any(acc):\n", "            logp[S0][acc] = logp_new[acc]\n", "            q[S0][acc]    = q_new[acc]\n", "\n", "    return q, logp\n", "\n", "#def metropolis_hastings(iLikelihood,iPrior,iProposal,iAcceptance,xinit,data,niterations):\n", "def metropolis_hastings_ensemble(xinit, logpinit, data, Npars, Nwalk, Nstep,iFunc):\n", "    samples = np.ndarray((Nwalk, Nstep, Npars))\n", "    samples[:, 0, :] = np.array(xinit)\n", "\n", "    lnprob = np.ndarray((Nwalk, Nstep))\n", "    lnprob[:, 0] = np.array(logpinit)\n", "\n", "    # Iterate over the Markov steps\n", "    for i in range(1, Nstep):\n", "        if (i % 500)==0:\n", "            print(\"Steps:\",i)\n", "        q, p = proposal(samples[:,i-1,:], lnprob[:,i-1], data, Nwalk, Npars, iFunc)\n", "        samples[:,i,:] = np.array(q)\n", "        lnprob[:,i] = np.array(p)\n", "    \n", "    return samples, lnprob\n", "\n", "# Set up MCMC parameters\n", "Npars = xinit.shape[0]\n", "Nwalk = 100\n", "Nstep = 2500 #larger Nstep used in related video\n", "\n", "def log_like(x,data,iFunc):\n", "    return -1.*np.sum((iFunc(x,data[:,0])-data[:,1])**2,axis=1)\n", "\n", "def func(x,t):\n", "    out=[]\n", "    for pX in x:\n", "        val=pX[0]*np.sin(pX[1]/10000.*t) +  pX[2]*np.sin(pX[3]/10000*t) +  pX[4]*np.sin(pX[5]/10000*t) + pX[6]\n", "        out.append(val)\n", "    out = np.array(out)\n", "    return out\n", "\n", "q0    = np.array([xinit + 1.0e-4*np.random.randn(Npars) for i in range(Nwalk)])\n", "logp0 = log_like(q0, data, func)\n", "accepted, likelihood = metropolis_hastings_ensemble(q0,logp0,data,Npars,Nwalk,Nstep,func)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["lect_03", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.3-runcell07\n", "\n", "lproj = np.max(likelihood,axis=0)\n", "plt.plot(lproj)\n", "plt.xlabel(\"iteration\")\n", "plt.ylabel(\"$\\mathcal{L}$\")\n", "plt.show()\n", "\n", "maxval=np.argmax(lproj)\n", "maxy=np.argmax(likelihood[:,maxval])\n", "bestpars=np.array([accepted[maxy,maxval]])\n", "output=func(bestpars,times)\n", "\n", "plt.plot(times[0:4000],amps[0:4000])\n", "plt.plot(times[0:4000],output.flatten()[0:4000])\n", "plt.xlabel('time (years)')\n", "plt.ylabel('temp (C)')\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["lect_03", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.3-runcell08\n", "\n", "def plotter(accepted,times,amps):\n", "    plt.ion()\n", "    plt.plot(times,amps,label='Ice Core Data')\n", "    output=func(accepted[:,-1],times)\n", "    for i0 in range(100):\n", "        plt.plot(times, output[i0], color=\"r\", alpha=0.1)\n", "    plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n", "    plt.xlabel('Years ago')\n", "    plt.ylabel(r'$\\Delta$ T (C)')\n", "    plt.legend()\n", "    plt.show()\n", "    \n", "plotter(accepted,times,amps)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "tags": ["lect_03", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.3-runcell09\n", "\n", "import corner \n", "labels = ['$a_{1}$','$\\omega_{1}$','$a_{2}$','$\\omega_{2}$','$a_{3}$','$\\omega_{3}$','A']\n", "samples=accepted[:,-500:-1]\n", "samples=np.reshape(samples,(samples.shape[0]*samples.shape[1],samples.shape[2]))\n", "print(samples.shape)\n", "fig = corner.corner(samples,show_titles=True,labels=labels,plot_datapoints=True,quantiles=[0.16, 0.5, 0.84])\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["lect_03", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.3-runcell10\n", "\n", "def prior(like,x):\n", "    #Adjust the likelihood by the prior\n", "    prior=np.ones(x.shape[0])\n", "    idx = np.where((x[:,1] < x[:,3]) | (x[:,3] < x[:,5]) | (x[:,1] < x[:,5]))\n", "    like[idx]+=np.log(prior[idx])\n", "    return like\n", "\n", "def mc_update(walkers, otherwalkers, logp0, Npars, idata, ifunc, a=2.0,iPrint=False):\n", "    Nw = len(walkers)\n", "    No = len(otherwalkers)\n", "\n", "    # Calculates random gradient for the affine linear transformation \n", "    Z = (((a - 1.0) * np.random.rand(Nw) + 1.0) ** 2.0) / a\n", "    rint = np.random.randint(No, size=(Nw,))\n", "    qt1 = otherwalkers[rint] - Z[:, np.newaxis] * (otherwalkers[rint] - walkers)    \n", "    # Calculate the posterior probability of the new position\n", "    logp1 = log_like(qt1, idata,ifunc)\n", "    logp1 = prior(logp1,qt1)\n", "    \n", "    #Now do the usual Markov update\n", "    #p_diff = (Npars - 1) * np.log(Z) + logp - logp0\n", "    p_diff = logp1 - logp0\n", "\n", "    rshape=np.random.rand(p_diff.shape[0])\n", "    accept = p_diff > np.log(np.random.rand(p_diff.shape[0]))\n", "    return qt1, logp1, accept\n", "\n", "Nwalk = 100\n", "Nstep = 2500 #larger Nstep used in related video\n", "q0    = np.array([xinit + 1.0e-4*np.random.randn(Npars) for i in range(Nwalk)])\n", "logp0 = log_like(q0, data, func)\n", "accepted, likelihood = metropolis_hastings_ensemble(q0,logp0,data,Npars,Nwalk,Nstep,func)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "tags": ["lect_03", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.3-runcell11\n", "\n", "import corner \n", "labels = ['$a_{1}$','$\\omega_{1}$','$a_{2}$','$\\omega_{2}$','$a_{3}$','$\\omega_{3}$','A']\n", "samples=accepted[:,-500:-1]\n", "samples=np.reshape(samples,(samples.shape[0]*samples.shape[1],samples.shape[2]))\n", "print(samples.shape)\n", "fig = corner.corner(samples,show_titles=True,labels=labels,plot_datapoints=True,quantiles=[0.16, 0.5, 0.84])\n", "\n", "plotter(accepted,times,amps)\n", "\n", "lproj = np.max(likelihood,axis=0)\n", "plt.plot(lproj)\n", "plt.xlabel(\"iteration\")\n", "plt.ylabel(\"$\\mathcal{L}$\")\n", "plt.show()\n", "\n", "maxval=np.argmax(lproj)\n", "maxy=np.argmax(likelihood[:,maxval])\n", "bestpars=np.array([accepted[maxy,maxval]])\n", "output=func(bestpars,times)\n", "\n", "plt.plot(times[0:4000],amps[0:4000])\n", "plt.plot(times[0:4000],output.flatten()[0:4000])\n", "plt.xlabel('time (years)')\n", "plt.ylabel('temp (C)')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["lect_03", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L22.3-runcell12\n", "\n", "xinit = np.array([2.,1.6,1.76,2.16,-0.1,10.6,-5.])\n", "\n", "def lmfunc(x,p0,p1,p2,p3,p4,p5,p6):\n", "    #val=pX[0]*np.sin(pX[1]/10000.*t) +  pX[2]*np.sin(pX[3]/10000*t) +  pX[4]*np.sin(pX[5]/10000*t) + pX[6]\n", "    return p0*np.sin(p1/10000.*x) +  p2*np.sin(p3/10000.*x) +  p4*np.sin(p5/10000.*x) + p6\n", "\n", "model  = lmfit.Model(lmfunc)\n", "print(data[:,1],data[:,0])\n", "params = model.make_params(p0=xinit[0],p1=xinit[1],p2=xinit[2],p3=xinit[3],p4=xinit[4],p5=xinit[5],p6=xinit[6])\n", "result = model.fit(data=data[:,1], params=params, x=data[:,0])\n", "result.plot()\n", "lmfit.report_fit(result)"]}, {"cell_type": "markdown", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='exercises_23_3'></a>     \n", "\n", "| [Top](#section_23_0) | [Restart Section](#section_23_3) |\n"]}, {"cell_type": "markdown", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 22.3.1</span>\n", "\n", "Run the fit with only 6 walkers, but still 2500 steps. What is the value of the likelihood that the code converges to? Think about how this compares to the convergence of the earlier code, where 100 walkers were used. Is this doing a good enough job?\n", "\n", "Run several trials and report your answer for the typical maximum likelihood as a number (it will be negative) with precision `1e4`.\n", "\n", "\n", "<br>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L22.3.1\n", "\n", "Nwalk = ###YOUR CODE HERE\n", "Nstep = 2500\n", "xinit = np.array([5.,1.,5.,2.,-5.,10.,-5.])\n", "q0    = np.array([xinit + 1.0e-4*np.random.randn(Npars) for i in range(Nwalk)])\n", "logp0 = log_like(q0, data, func)\n", "accepted, likelihood = metropolis_hastings_ensemble(q0,logp0,data,Npars,Nwalk,Nstep,func)\n", "\n", "lproj = np.max(likelihood,axis=0)\n", "max_likelihood = ###YOUR CODE HERE\n", "\n", "print(\"Maximum Liikelihood:\", max_likelihood)\n", "plt.plot(lproj)\n", "plt.xlabel(\"iteration\")\n", "plt.ylabel(\"$\\mathcal{L}$\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 22.3.2</span>\n", "\n", "Let's run MCMC on some data that has discontinuities, and see if we can get a good fit using a step-function. There are several steps to understanding and implementing this. In this problem we will describe the data and fit function, with the goal of defining a prior.\n", "\n", "<h3>Step 1: Generate Data</h3>\n", "\n", "Generate three sets of random data, that will ultimately create a tiered box shape. Run the following code to visualize this shape:\n", "\n", "<pre>\n", "np.random.seed(40)\n", "vals=np.random.rand(1000)*10\n", "vals=np.append(vals,np.random.rand(1000)*5 + 2.5)\n", "vals=np.append(vals,np.random.rand(1000)*2+4.)\n", "hist,bin_edges=np.histogram(vals,bins=np.arange(0,10.5,0.25))\n", "bin_centers=0.5*(bin_edges[:-1]+bin_edges[1:])\n", "plt.errorbar(bin_centers,hist,np.sqrt(hist),fmt='o', color='k')\n", "plt.xlabel(\"x\")\n", "plt.ylabel(\"events\")\n", "plt.show()\n", "</pre>\n", "\n", "<h3>Step 2: Define a Fit Function</h3>\n", "\n", "The function we will try to fit is given by:\n", "\n", "<pre>\n", "def func(x,t):\n", "    out=[]\n", "    for pX in x:\n", "        val=pX[0]*np.heaviside(t-pX[4],1) + \\\n", "            pX[1]*np.heaviside(t-pX[5],1) + \\\n", "            pX[2]*np.heaviside(t-pX[6],1) + \\\n", "            pX[3]*np.heaviside(t-pX[7],1) + pX[8]\n", "        out.append(val)\n", "    out = np.array(out)\n", "    return out\n", "</pre>\n", "\n", "Optionally, plot the fit function for some random parameter choices, to see what it looks like:\n", "\n", "<pre>\n", "# Define a single set of parameters\n", "# Format: [height1, height2, height3, height4, position1, position2, position3, position4, offset]\n", "params = np.array([\n", "    [2, -1, 3, -2, 1, 3, 5, 7, 0]  # Example parameter set\n", "])\n", "\n", "# Define the t values\n", "t_values = np.linspace(0, 10, 1000)  # t goes from 0 to 10 with 1000 points\n", "\n", "# Calculate the output of the function for this single set of parameters\n", "output = func(params, t_values)\n", "\n", "# Plot the result\n", "plt.figure(figsize=(10, 6))\n", "plt.plot(t_values, output[0], label=\"Parameter Set\")\n", "plt.xlabel('t')\n", "plt.ylabel('Function Value')\n", "plt.title('Plot of func for a Single Parameter Set')\n", "plt.legend()\n", "plt.grid(True)\n", "plt.show()\n", "</pre>\n", "\n", "\n", "<h3>Step 3: Define a Prior</h3>\n", "\n", "The prior places constraints on the parameters of the model. It will have the following form:\n", "\n", "<pre>\n", "def prior(like,x):\n", "    #Adjust the likelihood by the prior\n", "    prior=np.zeros(x.shape[0])\n", "    idx = np.where(###YOUR CODE HERE: ENTER CONSTRAINTS)\n", "    like[idx]+=np.log(prior[idx])\n", "    \n", "    #ADD OTHER CONSTRAINTS\n", "    idx = np.where(###YOUR CODE HERE: ENTER CONSTRAINTS)\n", "    like[idx]+=np.log(prior[idx])\n", "    return like\n", "</pre>\n", "\n", "\n", "**Here is where the question comes in.** Consider the constraints below. Select ALL options that are beneficial to add to the prior, based on the form of the data and model we are trying to fit. Note: some options may seem useful, but are not ideal because\n", "\n", "A) The step positions should be strictly increasing (i.e., each step occurs at a farther position than the previous one):\\\n", "`idx = np.where((x[:,4] > x[:,5]) | (x[:,5] > x[:,6]) | (x[:,6] > x[:,7]))`\n", "\n", "B) The step heights should be non-negative, meaning the model should not have any downward steps:\\\n", "`idx = np.where((x[:,0] < 0) | (x[:,1] < 0) | (x[:,2] < 0) | (x[:,3] < 0))`\n", "\n", "C) The difference between consecutive step positions should be at least 1 to ensure that the steps are well-separated:\\\n", "`idx = np.where((x[:,5] - x[:,4] < 1.0) | (x[:,6] - x[:,5] < 1.0) | (x[:,7] - x[:,6] < 1.0))`\n", "\n", "D) The step positions should be non-negative, meaning all steps should occur at positive positions:\\\n", "`idx = np.where((x[:,4] < 0) | (x[:,5] < 0) | (x[:,6] < 0) | (x[:,7] < 0))`\n", "\n", "E) The sum of the step heights should be equal to a specific value (e.g., 10) to ensure the total height of the function is fixed:\\\n", "`idx = np.where(np.abs(x[:,0] + x[:,1] + x[:,2] + x[:,3] - 10) > 1e-6)`\n", "\n", "F) The step heights should alternate in sign, ensuring that each step is followed by a drop, creating an oscillating pattern:\n", "`idx = np.where((x[:,0] * x[:,1] > 0) | (x[:,1] * x[:,2] > 0) | (x[:,2] * x[:,3] > 0))`\n", "\n", "G) The step positions should be within a specific range (e.g., between 1 and 5) to limit the function\u2019s domain:\\\n", "`idx = np.where((x[:,4] < 1) | (x[:,4] > 5) | (x[:,5] < 1) | (x[:,5] > 5) | (x[:,6] < 1) | (x[:,6] > 5) | (x[:,7] < 1) | (x[:,7] > 5))`\n", "\n", "H) The constant offset should be non-negative to ensure that the function does not drop below a certain baseline:\\\n", "`idx = np.where(x[:,8] < 0)`\n", "\n", "\n", "<br>"]}, {"cell_type": "markdown", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 22.3.3</span>\n", "\n", "Now run MCMC to fit the step-function that we have defined, and compare with fitting performed by lmfit. You will need to define the `prior` in the code below, based on your answer to the previous question. Which does a better job? \n", "\n", "A) MCMC is better.\\\n", "B) lmfit is better.\\\n", "C) They both perform the same.\n", "\n", "<br>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L22.3.3\n", "\n", "#Generate the data -> just run this code if \n", "#you want to see what it looks like first\n", "#-----------------------------------------------\n", "np.random.seed(40)\n", "vals=np.random.rand(1000)*10\n", "vals=np.append(vals,np.random.rand(1000)*5 + 2.5)\n", "vals=np.append(vals,np.random.rand(1000)*2+4.)\n", "hist,bin_edges=np.histogram(vals,bins=np.arange(0,10.5,0.25))\n", "bin_centers=0.5*(bin_edges[:-1]+bin_edges[1:])\n", "plt.errorbar(bin_centers,hist,np.sqrt(hist),fmt='o', color='k')\n", "plt.xlabel(\"x\")\n", "plt.ylabel(\"events\")\n", "plt.show()\n", "\n", "\n", "#define the step-function used for fitting\n", "#-----------------------------------------------\n", "def func(x,t):\n", "    out=[]\n", "    for pX in x:\n", "        val=pX[0]*np.heaviside(t-pX[4],1) + \\\n", "            pX[1]*np.heaviside(t-pX[5],1) + \\\n", "            pX[2]*np.heaviside(t-pX[6],1) + \\\n", "            pX[3]*np.heaviside(t-pX[7],1) + pX[8]\n", "        out.append(val)\n", "    out = np.array(out)\n", "    return out\n", "\n", "def log_like(x,data,iFunc):\n", "    return -1.*np.sum((iFunc(x,data[:,0])-data[:,1])**2,axis=1)\n", "\n", "\n", "#define the prior\n", "#-----------------------------------------------\n", "def prior(like,x):\n", "    #Adjust the likelihood by the prior\n", "    prior=np.zeros(x.shape[0])\n", "    idx = np.where(###YOUR CODE HERE: ENTER CONSTRAINTS)\n", "    like[idx]+=np.log(prior[idx])\n", "    \n", "    #ADD OTHER CONSTRAINTS\n", "    idx = np.where(###YOUR CODE HERE: ENTER CONSTRAINTS)\n", "    like[idx]+=np.log(prior[idx])\n", "    return like\n", "\n", "\n", "#redefine some functions that we have used already\n", "#-----------------------------------------------\n", "def mc_update(walkers, otherwalkers, logp0, Npars, idata, ifunc, a=2.0,iPrint=False):\n", "    Nw = len(walkers)\n", "    No = len(otherwalkers)\n", "\n", "    # Calculates random gradient for the affine linear transformation\n", "    Z = (((a - 1.0) * np.random.rand(Nw) + 1.0) ** 2.0) / a\n", "    rint = np.random.randint(No, size=(Nw,))\n", "    qt1 = otherwalkers[rint] - Z[:, np.newaxis] * (otherwalkers[rint] - walkers)\n", "    # Calculate the posterior probability of the new position\n", "    logp1 = log_like(qt1, idata,ifunc)\n", "    logp1 = prior(logp1,qt1)\n", "\n", "    #Now do the usual Markov update\n", "    #p_diff = (Npars - 1) * np.log(Z) + logp - logp0\n", "    p_diff = logp1 - logp0\n", "\n", "    rshape=np.random.rand(p_diff.shape[0])\n", "    accept = p_diff > np.log(np.random.rand(p_diff.shape[0]))\n", "    return qt1, logp1, accept\n", "\n", "\n", "def log_like(x,data,iFunc):\n", "    return -1.*np.sum((iFunc(x,data[:,0])-data[:,1])**2,axis=1)\n", "\n", "\n", "#run MCMC\n", "#-----------------------------------------------\n", "Npars = 9\n", "Nwalk = 100\n", "Nstep = 5000\n", "xinit = np.array([100,100,-100,-100,1,2,3,4,100])\n", "q0    = np.array([xinit + 1.0e-4*np.random.randn(Npars) for i in range(Nwalk)])\n", "data = np.vstack((bin_centers,hist))\n", "data = data.T\n", "logp0 = log_like(q0, data, func)\n", "accepted, likelihood = metropolis_hastings_ensemble(q0,logp0,data,Npars,Nwalk,Nstep,func)\n", "\n", "lproj = np.max(likelihood,axis=0)\n", "plt.plot(lproj)\n", "plt.xlabel(\"iteration\")\n", "plt.ylabel(\"$\\mathcal{L}$\")\n", "plt.show()\n", "\n", "maxval=np.argmax(lproj)\n", "maxy=np.argmax(likelihood[:,maxval])\n", "bestpars=np.array([accepted[maxy,maxval]])\n", "print(bestpars)\n", "output=func(bestpars,bin_centers)\n", "\n", "plt.errorbar(bin_centers,hist,np.sqrt(hist),fmt='o', color='k')\n", "plt.plot(bin_centers,output.flatten(),color='red')\n", "plt.xlabel(\"x\")\n", "plt.ylabel(\"events\")\n", "plt.show()\n", "\n", "\n", "#fit with lmfit\n", "#-----------------------------------------------\n", "def lmfunc(x,p0,p1,p2,p3,p4,p5,p6,p7,p8):\n", "    return p0*np.heaviside(x-p4,1) + p1*np.heaviside(x-p5,1) + p2*np.heaviside(x-p6,1) + p3*np.heaviside(x-p7,1)+p8\n", "\n", "model  = lmfit.Model(lmfunc)\n", "print(data[:,1],data[:,0])\n", "params = model.make_params(p0=xinit[0],p1=xinit[1],p2=xinit[2],p3=xinit[3],p4=xinit[4],p5=xinit[5],p6=xinit[6],p7=xinit[7],p8=xinit[8])\n", "result = model.fit(data=data[:,1], params=params, x=data[:,0])\n", "result.plot()\n", "lmfit.report_fit(result)"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}