{"cells": [{"cell_type": "markdown", "id": "1ee373c5", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 24: Simulation-based (Likelihood-free) Inference</h1>\n"]}, {"cell_type": "markdown", "id": "41345f3a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_24_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L24.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "fce9b388", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_24_1\">L24.1 Simple bump-on-power-law example and Explicit Likelihood</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_24_1\">L24.1 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_24_2\">L24.2 Implicit Likelihood Method 1: Approximate Bayesian Computation</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_24_2\">L24.2 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_24_3\">L24.3 Implicit Likelihood Method 2: Neural Likelihood-ratio Estimation</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_24_3\">L24.3 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_24_4\">L24.4 Implicit Likelihood Method 3: Neural Posterior Estimation</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_24_4\">L24.4 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_24_5\">L24.5 A more complicated example: distribution of point sources in a 2D image</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_24_5\">L24.5 Exercises</a></td>\n", "    </tr>\n", "</table>"]}, {"cell_type": "markdown", "id": "467638f0", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "Simulation-based inference (SBI) is a powerful class of methods for performing inference in settings where the likelihood is computationally intractable, but simulations can be used in order to optimize the fitting of more complex models to data. We will use the term \"forward model\" to indicate generating simulated data given a set of input parameters, a process which will be called \"realizing\" a sample. In contrast, inference is the process of finding optimal values of the parameters given a set of data, and so can be considered working the model \"backwards\".\n", "\n", "In this Lesson, we will\n", "- Introduce the notion of an implicit likelihood, and how to leverage it to perform inference;\n", "- Look at an alternative, more \"traditional\" method for likelihood-free inference, Approximate Bayesian Computation (ABC);\n", "- Consider two common modern _neural_ SBI techniques: neural likelihood-ratio estimation (NRE) and neural posterior estimation (NPE);\n", "- Introduce the concept of statistical coverage testing and calibration.\n", "\n", "As examples, we will look at two very different situations. The first is the case where we are looking for a new particle which, if present, will generate an excess of events (a \"bump\") on top of a background of other known processes. In this case, our model will be a simple Gaussian signal on top of a background assumed to be a power law distribution. For this so-called \"bump hunt\", calculating the likelihood is computationally tractable, so we can compare a more traditional method with the approaches required in more complex situations.\n", "\n", " The second example is more complicated, requiring us to model a distribution of counts over a 2D space representing the locations of point sources. Here, the likelihood is computationally intractable. We will emphasize what it means for a likelihood to be computationally intractable/challenging and where the advantages of SBI come in.\n"]}, {"cell_type": "markdown", "id": "116c757a", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Installing Tools</h3>\n", "\n", "Before we do anything, let's make sure we install the tools we need. Note that this is another longer-than-typical install."]}, {"cell_type": "code", "execution_count": null, "id": "d59f3ad0", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.0-runcell00\n", "\n", "!pip install --upgrade emcee corner pytorch-lightning tqdm nflows"]}, {"cell_type": "markdown", "id": "dff0575f", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Next, run the cell below to import the relevant libraries for this notebook."]}, {"cell_type": "code", "execution_count": null, "id": "0f3a0154", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.0-runcell01\n", "\n", "import torch                            #https://pytorch.org/docs/stable/torch.html\n", "import torch.nn as nn                   #https://pytorch.org/docs/stable/nn.html\n", "import torch.nn.functional as F         #https://pytorch.org/docs/stable/nn.functional.html\n", "from torch.utils.data import TensorDataset, DataLoader, random_split  #https://pytorch.org/docs/stable/data.html\n", "import numpy as np                      #https://numpy.org/doc/stable/\n", "import emcee                            #https://emcee.readthedocs.io/en/stable/\n", "from scipy.stats import poisson         #https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.poisson.html\n", "from scipy.stats import chi2            #https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2.html\n", "from scipy.optimize import basinhopping #https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.basinhopping.html\n", "from tqdm import tqdm                   #https://pypi.org/project/tqdm/\n", "import matplotlib.pyplot as plt         #https://matplotlib.org/stable/api/pyplot_summary.html#module-matplotlib.pyplot\n", "import pytorch_lightning as pl          #https://lightning.ai/docs/pytorch/stable/\n", "import corner                           #https://corner.readthedocs.io/en/latest/\n"]}, {"cell_type": "markdown", "id": "3d515076", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Setting Default Figure Parameters</h3>\n", "\n", "The following code cell sets default values for figure parameters."]}, {"cell_type": "code", "execution_count": null, "id": "97d0edf8", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.0-runcell02\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title"]}, {"cell_type": "markdown", "id": "e198f21b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_24_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L24.1 Simple bump-on-power-law example and Explicit Likelihood</h2>  \n", "\n", "| [Top](#section_24_0) | [Previous Section](#section_24_0) | [Exercises](#exercises_24_1) | [Next Section](#section_24_2) |\n"]}, {"cell_type": "markdown", "id": "00dcb7cd", "metadata": {"tags": ["learner", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.3x+3T2023/block-v1:MITxT+8.S50.3x+3T2023+type@sequential+block@seq_LS24/block-v1:MITxT+8.S50.3x+3T2023+type@vertical+block@vert_LS24_vid1\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "03d20db6", "metadata": {"tags": ["learner", "md", "lect_01"]}, "source": ["<h3>Overview</h3>\n", "\n", "The **likelihood function** (see below), a key component in statistical inference, represents the probability of data given model parameters, $p(x|\\theta)$. However, calculating the likelihood can be challenging, especially for complex observations. This is because it involves computing the joint likelihood, $p(x, z|\\theta)$, where $z$ represents other variables needed to describe the data but are not of direct interest. Integrating over the space of $z$'s can be computationally intractable. Simulation-based inference offers a set of methods to approximate this calculation.\n", "\n", "\n", "<p align=\"center\">\n", "<img alt=\"the likelihood function\" src=\"https://raw.githubusercontent.com/mitx-8s50/images/main/L24/likelihood.png\" width=\"800\"/>\n", "</p>\n", "\n", "Simulators are useful for generating observations from given parameters by sampling from the likelihood distribution, $p(x|\\theta)$. **Simulation-based inference (SBI)** methods facilitate inference in the opposite direction, from observations to parameters.\n", "\n", "<p align=\"center\">\n", "<img alt=\"simulation-based inference\" src=\"https://raw.githubusercontent.com/mitx-8s50/images/main/L24/sbi.png\" width=\"800\"/>\n", "</p>\n", "\n", "Simulators excel at prediction tasks, generating data realizations through various random states. However, **simulators are poor for inference** because they require integrating over all possible intermediate latent states, which is challenging.\n", "\n", "<p align=\"center\">\n", "<img alt=\"prediction\" src=\"https://raw.githubusercontent.com/mitx-8s50/images/main/L24/prediction.png\" width=\"600\"/>\n", "</p>\n", "\n", "<p align=\"center\">\n", "<img alt=\"inference\" src=\"https://raw.githubusercontent.com/mitx-8s50/images/main/L24/inference.png\" width=\"600\"/>\n", "</p>\n", "\n"]}, {"cell_type": "markdown", "id": "37c96ada", "metadata": {"tags": ["learner", "md", "lect_01"]}, "source": ["<h3>Why are cases like this interesting scientifically?</h3>\n", "\n", "Inference from complex, high-dimensional scientific data typically makes use of \"summary statistics\" -- lower-dimensional versions of the data that are easier to work with.\n", "\n", "SBI allows us to work with data and models at their full complexity.\n", "\n", "<p align=\"center\">\n", "<img alt=\"summaries\" src=\"https://raw.githubusercontent.com/mitx-8s50/images/main/L24/summaries.png\" width=\"800\"/>\n", "</p>\n"]}, {"cell_type": "markdown", "id": "a49e0237", "metadata": {"tags": ["lect_01", "learner"]}, "source": ["<h3>An Example</h3>\n", "\n", "As an initial example, consider a Gaussian signal $x_s$ parameterized by {amplitude $A_s$, mean location $\\mu_s$, and standard deviation $\\sigma_s$} on top of a power law background $x_b$ parameterized by {amplitude $A_b$ and power-law exponent $n_b$}. The total counts in any bin $x$ will be a Poisson distribution with a mean of $x_s+x_b$.\n", "\n", "$$ x_b = A_b\\,y^{n_b}$$\n", "\n", "$$x_s = A_s\\,\\exp^{-(y - \\mu_s)^2 / 2\\sigma_s^2}$$\n", "\n", "$$x \\sim \\mathrm{Pois}(x_b + x_s)$$"]}, {"cell_type": "code", "execution_count": null, "id": "87140346", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.1-runcell01\n", "\n", "def bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b):\n", "    \"\"\" Forward model for a Gaussian bump (amp_s, mu_s, std_s) on top of a power-law background (amp_b, exp_b).\n", "    \"\"\"\n", "    x_b = amp_b * (y ** exp_b)  # Power-law background\n", "    x_s = amp_s * np.exp(-((y - mu_s) ** 2) / (2 * std_s ** 2))  # Gaussian signal\n", "\n", "    x = x_b + x_s  # Total mean signal\n", "\n", "    return x"]}, {"cell_type": "code", "execution_count": null, "id": "73923141", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.1-runcell02\n", "\n", "#instead of sqrt(N) uncertainties, get the full asymmetric uncertainty using a chi2\n", "def poisson_interval(k, alpha=0.32): \n", "    \"\"\" Uses chi2 to get the poisson interval.\n", "    \"\"\"\n", "    a = alpha\n", "    low, high = (chi2.ppf(a/2, 2*k) / 2, chi2.ppf(1-a/2, 2*k + 2) / 2)\n", "    if k == 0: \n", "        low = 0.0\n", "    return k - low, high - k\n", "\n", "y = np.linspace(0.1, 1, 50)  # Dependent variable\n", "\n", "# Mean expected counts\n", "x_mu = bump_forward_model(y, \n", "                    amp_s=50, mu_s=0.8, std_s=0.05,  # Signal params\n", "                    amp_b=50, exp_b=-0.5)  # Background params\n", "\n", "# Realized counts\n", "np.random.seed(42)\n", "x = np.random.poisson(x_mu)\n", "x_err = np.array([poisson_interval(k) for k in x.T]).T #getting asymmetric errorbars\n", "\n", "# Plot\n", "plt.plot(y, x_mu, color='k', ls='--', label=\"Mean expected counts\")\n", "plt.errorbar(y, x, yerr=x_err, fmt='o', color='k', label=\"Realized counts\")\n", "\n", "plt.xlabel(\"$y$\")\n", "plt.ylabel(\"Counts\")\n", "\n", "plt.legend()"]}, {"cell_type": "markdown", "id": "0ed37a00", "metadata": {"tags": ["lect_01", "learner"]}, "source": ["<h3>The Explicit Likelihood</h3>\n", "\n", "In this case, we can write down a log-likelihood as a Poisson with a mean $\\mu$ given by the prediction returned by the forward model, evaluated at a value given by the data $x$. Here, as in the figures shown above, we use the vector `theta` ($\\theta$ in the figures) to denote the set of model parameters."]}, {"cell_type": "code", "execution_count": null, "id": "d509b2c2", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.1-runcell03\n", "\n", "def log_like(theta, y, x):\n", "    \"\"\" Log-likehood function for a Gaussian bump (amp_s, mu_s, std_s) on top of a power-law background (amp_b, exp_b).\n", "    \"\"\"\n", "    amp_s, mu_s, std_s, amp_b, exp_b = theta\n", "    mu = bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b)\n", "    return poisson.logpmf(x, mu).sum()"]}, {"cell_type": "markdown", "id": "2966a14b", "metadata": {"tags": ["lect_01", "learner"]}, "source": ["Let's focus on just 2 parameters for simplicity, the signal amplitude and mean location. We compute the log-likelihood below, where we expect a number that is high for parameters that are compatible with the data, and low for parameters that are not compatible."]}, {"cell_type": "code", "execution_count": null, "id": "31c4a7d2", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.1-runcell04\n", "\n", "def log_like_sig(params, y, x):\n", "    \"\"\" Log-likehood function for a Gaussian bump (amp_s, mu_s) on top of a fixed PL background.\n", "    \"\"\"\n", "    amp_s, mu_s = params\n", "    std_s, amp_b, exp_b = 0.05, 50, -0.5\n", "    mu = bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b)\n", "    return poisson.logpmf(x, mu).sum()\n", "\n", "log_like_sig([50, 0.8], y, x)"]}, {"cell_type": "markdown", "id": "7531d766", "metadata": {"tags": ["lect_01", "learner"]}, "source": ["Now we calculate a maximum-liklelihood estimate (MLE). Note, we want to minimize the negative log-likelihood, and we do so using the *basinhopping algorithm*. Basin-hoppig is an alternative approach to gradient descent. Like MCMC appraochs, it does not compute gradients rather it randomly samples points until the lowest negative log-likelihood is found."]}, {"cell_type": "code", "execution_count": null, "id": "575b15f6", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.1-runcell05\n", "\n", "# Initial guess for the parameters\n", "initial_guess = [100., 0.1]\n", "\n", "# Set up the minimizer_kwargs for the basinhopping algorithm\n", "minimizer_kwargs = {\"method\": \"L-BFGS-B\", \"bounds\": ((0, 200), (0, 1))}\n", "\n", "# Perform the optimization using basinhopping\n", "opt = basinhopping(lambda thetas: -log_like_sig(thetas, y, x), initial_guess, minimizer_kwargs=minimizer_kwargs)\n", "\n", "print(\"MLE parameters: {}; true parameters: {}\".format(opt.x, (50, 0.8)))\n"]}, {"cell_type": "markdown", "id": "1f199c0f", "metadata": {"tags": ["lect_01", "learner"]}, "source": ["Then, approximate the posterior using `emcee`, a package which implements a Markov Chain Monte Carlo (MCMC) procedure.\n", "\n", "Recall Bayes' theorem:\n", "\n", "$$ p(\\theta\\mid x) = \\frac{p(x\\mid\\theta)\\,p(\\theta)}{p(x)}$$\n", "\n", "With MCMC, we sample from the un-normalized posterior distribution, bypassing the need to compute the normalization $p(x)$ which is fixed for a given set of data $x$. We therefore specify the log-density $\\log{p(x\\mid\\theta)} + \\log{p(\\theta)}$.\n"]}, {"cell_type": "code", "execution_count": null, "id": "c369f9ef", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.1-runcell06\n", "\n", "def log_prior(thetas):\n", "    \"\"\" Log-prior function for a Gaussian bump (amp_s, mu_s) on top of a fixed PL background.\n", "    \"\"\"\n", "    amp_s, mu_s = thetas\n", "    if 0 < amp_s < 200 and 0 < mu_s < 2:\n", "        return 0\n", "    else:\n", "        return -np.inf\n", "    \n", "def log_post(thetas, y, x):\n", "    \"\"\" Log-posterior function for a Gaussian bump (amp_s, mu_s) on top of a fixed PL background.\n", "    \"\"\"\n", "    lp = log_prior(thetas)\n", "    if not np.isfinite(lp):\n", "        return -np.inf\n", "    else:\n", "        return lp + log_like_sig(thetas, y, x)\n", "    \n", "# Sampling with `emcee`\n", "ndim, nwalkers = 2, 32\n", "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_post, args=(y, x))\n", "\n", "pos = opt.x + 1e-3 * np.random.randn(nwalkers, ndim)\n", "sampler.run_mcmc(pos, 5000, progress=True);"]}, {"cell_type": "markdown", "id": "e8ab4876", "metadata": {"tags": ["lect_01", "learner", "md"]}, "source": ["To examine the results of our analysis, and compare with the true values, the following code cell generates a **corner plot**. Corner plots, which we've seen in previous lessons, are used to visualize the posterior distributions of parameters and their correlations. Specifically, the following plot shows:\n", "\n", "- the \"true\" input values used to generate the data, represented by the lines as $A_s=50$ and $\\mu_s=0.80$\n", "- 1D histograms corresponding to the marginal distributions of each parameter: $A_s$ (`amp_s`) and $\\mu_s$ (`mu_s`)\n", "- a 2D pontour plot showing the joint distributions between the parameters $A_s$ and $\\mu_s$\n", "- note, the 2D contour plots includes confidence intervals (68%, 95%, and 99% levels) to indicate the density of the parameter combinations\n", "\n", "As you can see, the position of the peak is matched very closely, but the inferred amplitude is off by about 7%. Also note that the contours in the 2D plot are very close to circular. This means that the amplitude and mean are largely uncorrelated when fitting the model parameters."]}, {"cell_type": "code", "execution_count": null, "id": "30032fe3", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.1-runcell07\n", "\n", "# Plot posterior samples\n", "flat_samples = sampler.get_chain(discard=1000, flat=True)\n", "\n", "#make the corner plot with true values as lines\n", "corner.corner(flat_samples, labels=[\"amp_s\", \"mu_s\"], truths=[50, 0.8], smooth=.1);\n"]}, {"cell_type": "markdown", "id": "d6ad3f5d", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_24_1'></a>     \n", "\n", "| [Top](#section_24_0) | [Restart Section](#section_24_1) | [Next Section](#section_24_2) |\n"]}, {"cell_type": "markdown", "id": "24b1aca7", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 24.1.1</span>\n", "\n", "The calculations in this section only inferred two of the signal parameters, $A_s$ (`amp_s`) and $\\mu_s$ (`mu_s`). Complete the code below to also infer $\\sigma_s$ (`std_s`) via likelihood maximization (negative log-likelihood minimization). Report your result for the value of $\\sigma_s$ as a number with precision `1e-4`.\n", "\n", "<br>"]}, {"cell_type": "code", "execution_count": null, "id": "df56c003", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L24.1.1\n", "\n", "def log_like_sig2(params, y, x):\n", "    #Log-likehood function for a Gaussian bump (amp_s, mu_s, std_s) on top of a fixed PL background.\n", "    \n", "    #YOUR CODE HERE: make std_s one of the params\n", "    return poisson.logpmf(x, mu).sum()\n", "\n", "\n", "# Initial guess for the parameters\n", "initial_guess = [100., 0.1, 0.01]\n", "\n", "# Set up the minimizer_kwargs for the basinhopping algorithm\n", "minimizer_kwargs = {\"method\": \"L-BFGS-B\", \"bounds\": ((0, 200), (0, 1), (0.001, .2))}\n", "\n", "# Perform the optimization using basinhopping\n", "opt2 = basinhopping(lambda thetas: -log_like_sig2(thetas, y, x), initial_guess, minimizer_kwargs=minimizer_kwargs)\n", "\n", "print(\"MLE parameters: {}; true parameters: {}\".format(opt2.x, (50, 0.8, 0.05)))"]}, {"cell_type": "markdown", "id": "787806e2", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 24.1.2</span>\n", "\n", "Now generate the posterior distributions by running MCMC on the 3 parameters, as we did above for the two parameters. Make a corner plot, as done at the end of this section. Which parameters appear to be correlated?\n", "\n", "A) `amp_s` and `mu_s`\n", "\n", "B) `amp_s` and `std_s`\n", "\n", "C) `mu_s` and `std_s`\n", "\n", "<br>"]}, {"cell_type": "code", "execution_count": null, "id": "f03bb956", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L24.1.2\n", "\n", "def log_prior2(thetas):\n", "    #Log-prior function for a Gaussian bump (amp_s, mu_s, std_s) on top of a fixed PL background.\n", "    \n", "    #YOUR CODE HERE\n", "\n", "def log_post2(thetas, y, x):\n", "    #Log-posterior function for a Gaussian bump (amp_s, mu_s, std_s) on top of a fixed PL background.\n", "    \n", "    #YOUR CODE HERE\n", "    \n", "    \n", "# Sampling with `emcee`\n", "ndim, nwalkers = 3, 32\n", "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_post2, args=(y, x))\n", "\n", "# using basinhopping optimization from EXERCISE: L24.1.1 (i.e., opt2)\n", "pos = opt2.x + 1e-3 * np.random.randn(nwalkers, ndim)\n", "sampler.run_mcmc(pos, 5000, progress=True);\n", "\n", "# Plot posterior samples\n", "#YOUR CODE HERE"]}, {"cell_type": "markdown", "id": "e2786b06", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_24_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L24.2 Implicit Likelihood Method 1: Approximate Bayesian Computation</h2>  \n", "\n", "| [Top](#section_24_0) | [Previous Section](#section_24_1) | [Exercises](#exercises_24_2) | [Next Section](#section_24_3) |\n"]}, {"cell_type": "markdown", "id": "05175628", "metadata": {"tags": ["learner", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.3x+3T2023/block-v1:MITxT+8.S50.3x+3T2023+type@sequential+block@seq_LS24/block-v1:MITxT+8.S50.3x+3T2023+type@vertical+block@vert_LS24_vid2\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "e43adf1d", "metadata": {"tags": ["lect_02", "learner"]}, "source": ["<h3>Overview</h3>\n", "\n", "Now, we will preform the inference again, but this time without relying on the explicit likelihood evaluation. The key realization is that samples from the forward model implicitly encode the likelihood; when we are simulating data points $x$ for different parameter points $\\theta$, we are drawing samples from the likelihood:\n", "\n", "$$x\\sim p(x\\mid\\theta)$$\n", "\n", "which is where the _implicit_ aspect comes from. Let's write down a bump simulator:"]}, {"cell_type": "code", "execution_count": null, "id": "aa56d949", "metadata": {"tags": ["lect_02", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.2-runcell01\n", "\n", "def bump_simulator(thetas, y):\n", "    \"\"\" Simulate samples from the bump forward model given theta = (amp_s, mu_s) and abscissa points y.\n", "    \"\"\"\n", "    amp_s, mu_s = thetas\n", "    std_s, amp_b, exp_b = 0.05, 50, -0.5\n", "    x_mu = bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b)\n", "    x = np.random.poisson(x_mu)\n", "    return x\n", "\n", "#we had defined this funciton previously\n", "def bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b):\n", "    \"\"\" Forward model for a Gaussian bump (amp_s, mu_s, std_s) on top of a power-law background (amp_b, exp_b).\n", "    \"\"\"\n", "    x_b = amp_b * (y ** exp_b)  # Power-law background\n", "    x_s = amp_s * np.exp(-((y - mu_s) ** 2) / (2 * std_s ** 2))  # Gaussian signal\n", "\n", "    x = x_b + x_s  # Total mean signal\n", "\n", "    return x\n", "\n", "# Test it out\n", "y = np.linspace(0.1, 1, 50)\n", "bump_simulator([50, 0.8], y)"]}, {"cell_type": "markdown", "id": "f6913a95", "metadata": {"tags": ["lect_02", "learner"]}, "source": ["<h3>Approximate Bayesian Computation (ABC)</h3>\n", "\n", "The idea behind Approximate Bayesian Computation (ABC) is to realize samples from the forward model (with the parameters $\\theta$ drawn from a prior) and compare it to the dataset of interest $x$. If the data and realized samples are close enough to each other according to some criterion, we keep the parameter points."]}, {"cell_type": "markdown", "id": "7cd02ac6", "metadata": {"tags": ["lect_02", "learner"]}, "source": ["<p align=\"center\">\n", "<img alt=\"approximate bayesian computation\" src=\"https://raw.githubusercontent.com/mitx-8s50/images/main/L24/abc.png\" width=\"800\"/>\n", "</p>"]}, {"cell_type": "markdown", "id": "e4bbe91c", "metadata": {"tags": ["lect_02", "learner"]}, "source": ["The comparison criterion here is a simple mean squares error (MSE, denoted `eps` in the code shown below) between the data points and the output of the forward model with a given set of parameters. Play around with the parameters of the forward model (the inputs to the forward model are initially set to 50 and 0.8) to see how the criterion `eps` changes. You can also try simply rerunning code cell `L24.2-runcell03` repeatedly to see how the MSE changes for different independently realized samples."]}, {"cell_type": "code", "execution_count": null, "id": "49536412", "metadata": {"tags": ["lect_02", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.2-runcell02\n", "\n", "#define the simulated data again, if you have not done so in the previous section\n", "\n", "def poisson_interval(k, alpha=0.32): \n", "    \"\"\" Uses chi2 to get the poisson interval.\n", "    \"\"\"\n", "    a = alpha\n", "    low, high = (chi2.ppf(a/2, 2*k) / 2, chi2.ppf(1-a/2, 2*k + 2) / 2)\n", "    if k == 0: \n", "        low = 0.0\n", "    return k - low, high - k\n", "\n", "# Mean expected counts\n", "x_mu = bump_forward_model(y, \n", "                    amp_s=50, mu_s=0.8, std_s=0.05,  # Signal params\n", "                    amp_b=50, exp_b=-0.5)  # Background params\n", "\n", "# Realized counts\n", "np.random.seed(42)\n", "x = np.random.poisson(x_mu)\n", "x_err = np.array([poisson_interval(k) for k in x.T]).T"]}, {"cell_type": "code", "execution_count": null, "id": "f22c64e1", "metadata": {"tags": ["lect_02", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.2-runcell03\n", "\n", "x_fwd = bump_simulator([50, 0.8], y)\n", "eps = np.sum(np.abs(x - x_fwd) ** 2) / len(x)\n", "eps"]}, {"cell_type": "markdown", "id": "4513d69d", "metadata": {"tags": ["lect_02", "learner"]}, "source": ["<h3>Implementing the ABC Procedure</h3>\n", "\n", "Code cell `L24.2-runcell04` implements the ABC procedure by generating a lot of such independent samples, keeping ones that have an MSE below a specified threshold.\n", "\n", "Code cell `L24.2-runcell05` displays similar posterior distributions as were shown above. Specifically, the red distribution represents the MCMC results from before (narrower), while the black distribution shows the approximate posterior using ABC (broader). The ABC posterior roughly captures the central value but is more conservative, which is expected since it's based on simulations and isn't a highly precise method.\n", "\n", "Finally, note that the change of axes scales means that the contours are ovals instead of circles, but the lack of any tilt of these contours means that once again the probabilities for the amplitude and mean are independent."]}, {"cell_type": "code", "execution_count": null, "id": "dea9c014", "metadata": {"tags": ["lect_02", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.2-runcell04\n", "\n", "def abc(y, x, eps_thresh=500., n_samples=1000):\n", "    \"\"\"ABC algorithm for Gaussian bump model.\n", "\n", "    Args:\n", "        y (np.ndarray): Abscissa points.\n", "        x (np.ndarray): Data counts.\n", "        eps_thresh (float, optional): Acceptance threshold. Defaults to 500.0.\n", "        n_samples (int, optional): Number of samples after which to stop. Defaults to 1000.\n", "\n", "    Returns:\n", "        np.ndarray: Accepted samples approximating the posterior p(theta|x).\n", "    \"\"\"\n", "    samples = []\n", "    total_attempts = 0\n", "    progress_bar = tqdm(total=n_samples, desc=\"Accepted Samples\", unit=\"samples\")\n", "\n", "    # Keep simulating until we have enough accepted samples\n", "    while len(samples) < n_samples:\n", "        params = np.random.uniform(low=[0, 0], high=[200, 1])  # Priors; theta ~ p(theta)\n", "        x_fwd = bump_simulator(params, y)  # x ~ p(x|theta)\n", "        eps = np.sum(np.abs(x - x_fwd) ** 2) / len(x)  # Distance metric; d(x, x_fwd)\n", "        total_attempts += 1\n", "\n", "        # If accepted, add to samples\n", "        if eps < eps_thresh:\n", "            samples.append(params)\n", "            progress_bar.update(1)\n", "            acceptance_ratio = len(samples) / total_attempts\n", "            progress_bar.set_postfix(acceptance_ratio=f\"{acceptance_ratio:.3f}\")\n", "\n", "    progress_bar.close()\n", "    return np.array(samples)\n", "\n", "n_samples = 5_000\n", "post_samples = abc(y, x, eps_thresh=200, n_samples=n_samples)"]}, {"cell_type": "code", "execution_count": null, "id": "4905bd0b", "metadata": {"tags": ["lect_02", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.2-runcell05\n", "\n", "#note: here we also plot `flat_samples`, which was defined in cell `L24.1-runcell07`\n", "fig = corner.corner(post_samples, labels=[\"amp_s\", \"mu_s\"], truths=[50, 0.8], range=[(0, 200), (0.3, 1)], bins=50);\n", "corner.corner(flat_samples, labels=[\"amp_s\", \"mu_s\"], truths=[50, 0.8], fig=fig, color=\"red\", weights=np.ones(len(flat_samples)) * n_samples / len(flat_samples), range=[(0, 200), (0.3, 1)], bins=50);"]}, {"cell_type": "markdown", "id": "dcaa64e8", "metadata": {"tags": ["lect_02", "learner"]}, "source": ["<h3>Downsides</h3>\n", "\n", "Downsides of basic, plain vanilla, ABC:\n", "- ABC works well for low-dimensional data, but this raises the question \"How to summarize the data?\". And when reducing dimensionality, we inherently lose information.\n", "- Since there is no likelihood, comparison with data can be made through MSE, but can also be done other ways (some better or worse).\n", "- Choosing an exception threshold (epsilon) is tricky\u2014it must be small enough to be meaningful but not too small, or the process will take too long.\n", "- The process needs to be repeated for each new dataset or prior. "]}, {"cell_type": "markdown", "id": "1fc20d8c", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='exercises_24_2'></a>     \n", "\n", "| [Top](#section_24_0) | [Restart Section](#section_24_2) | [Next Section](#section_24_3) |\n"]}, {"cell_type": "markdown", "id": "8db642b6", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 24.2.1</span>\n", "\n", "We can attempt to use our knowledge of MSE to yield a more biased approach to finding the best fit parameters. What we can do is weight the parameers by `1./eps`. Thus, larger values of `eps`, that are closer to acceptance threshold, will be weighted LESS. Complete the code below in order to make corner plots. What happens to the best fit parameters?\n", "\n", "A) The algorithm finds the same best fit parameters.\\\n", "B) The algorithm cannot find the best fit parameters.\\\n", "C) The algorithm finds a narrower range for `amp_s` than `mu_s`.\\\n", "D) The algorithm finds a narrower range for `mu_s` than `amp_s`.\n", "\n", "<br>"]}, {"cell_type": "code", "execution_count": null, "id": "7277ff9a", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L24.2.1\n", "\n", "def abc_weight(y, x, n_samples=1000,eps_thresh=500):\n", "    #ABC algorithm for Gaussian bump model.\n", "    #\n", "    #Args:\n", "    #    y (np.ndarray): Abscissa points.\n", "    #    x (np.ndarray): Data counts.\n", "    #    eps_thresh (float, optional): Acceptance threshold. Defaults to 500.0.\n", "    #    n_samples (int, optional): Number of samples after which to stop. Defaults to 1000.\n", "    #\n", "    #Returns:\n", "    #    np.ndarray: Accepted samples approximating the posterior p(theta|x).\n", "\n", "    samples = []\n", "    weights = []\n", "    total_attempts = 0\n", "    progress_bar = tqdm(total=n_samples, desc=\"Accepted Samples\", unit=\"samples\")\n", "\n", "    # Keep simulating until we have enough accepted samples\n", "    while len(samples) < n_samples:\n", "        params = np.random.uniform(low=[0, 0], high=[200, 1])  # Priors; theta ~ p(theta)\n", "        x_fwd = bump_simulator(params, y)  # x ~ p(x|theta)\n", "        eps = np.sum(np.abs(x - x_fwd) ** 2) / len(x)  # Distance metric; d(x, x_fwd)\n", "        total_attempts += 1\n", "\n", "        # If accepted, add to samples\n", "        if eps < eps_thresh:\n", "            samples.append(params)\n", "            weights.append(#YOUR CODE HERE) #choose weights proportional to 1/eps\n", "            progress_bar.update(1)\n", "            acceptance_ratio = len(samples) / total_attempts\n", "            progress_bar.set_postfix(acceptance_ratio=f\"{acceptance_ratio:.3f}\")\n", "\n", "    progress_bar.close()\n", "    return np.array(samples), np.array(weights)*len(samples)/np.sum(weights)\n", "\n", "n_samples = 5_000\n", "post_samples_weight,sample_weights = abc_weight(y, x, n_samples=n_samples)\n", "\n", "fig = corner.corner(post_samples_weight,weights=sample_weights, labels=[\"amp_s\", \"mu_s\"], truths=[50, 0.8], range=[(0, 200), (0.3, 1)], bins=50);\n", "corner.corner(flat_samples, labels=[\"amp_s\", \"mu_s\"], truths=[50, 0.8], fig=fig, color=\"red\", weights=np.ones(len(flat_samples)) * n_samples / len(flat_samples), range=[(0, 200), (0.3, 1)], bins=50);\n"]}, {"cell_type": "markdown", "id": "a08284ad", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_24_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L24.3 Implicit Likelihood Method 2: Neural Likelihood-ratio Estimation</h2>  \n", "\n", "| [Top](#section_24_0) | [Previous Section](#section_24_2) | [Exercises](#exercises_24_3) | [Next Section](#section_24_4) |\n"]}, {"cell_type": "markdown", "id": "3b422f0f", "metadata": {"tags": ["learner", "lect_03"]}, "source": ["<h3>Neural Likelihood-ratio Estimation (NRE)</h3>\n", "\n", "Neural likelihood-ratio estimation uses a \"likelihood-ratio trick\" to convert a classifier into a likelihood ratio, which is widely useful, especially in fields like high-energy physics and cosmology. The classifier assigns a decision function (a confidence score between 0 and 1) that corresponds to the likelihood ratio between two classes (e.g., cat vs. bunny).\n", "\n", "For parameter estimation, where parameters are continuous, the classifier can be parameterized with these continuous values, allowing it to output a likelihood ratio. To simplify and stabilize the process, the denominator hypothesis can be defined as a marginal, leading to a likelihood-to-evidence ratio, which will be described more below."]}, {"cell_type": "markdown", "id": "52d2a093", "metadata": {"tags": ["learner", "lect_03"]}, "source": ["<p align=\"center\">\n", "<img alt=\"neural likelihood-ratio estimation\" src=\"https://raw.githubusercontent.com/mitx-8s50/images/main/L24/nre.png\" width=\"800\"/>\n", "</p>"]}, {"cell_type": "markdown", "id": "855cbb94", "metadata": {"tags": ["learner", "lect_03"]}, "source": ["For numerical stability, the alternate hypothesis $\\theta_0$ can be assumed to be one where $x$ and $\\theta$ are not correlated, i.e., drawn from the individual marginal distributions $\\{x, \\theta\\} \\sim p(x)\\,p(\\theta)$. Then, the alternate has support over the entire parameter space, instead of being a single hypothesis $\\theta_0$.\n", "\n", "In this case, we get the likelihood-to-evidence ratio,\n", "\n", "$$\\hat r(x, \\theta) = \\frac{s(x, \\theta)}{1 - s(x, \\theta)} = \\frac{p(x,\\theta)}{p(x)p(\\theta)} = \\frac{p(x\\mid\\theta)}{p(x)}$$\n", "\n", "Let's further define this in the language of machine learning. The expression $\\hat{r}(x, \\theta)$ is known as the **classifier logit**, which is the raw output of a classifier. This raw output is converted to a probability by a **softmax function**. After applying softmax, the **decision function** yields values between 0 and 1, indicating the likelihood of a given input belonging to a certain class.\n", "\n"]}, {"cell_type": "markdown", "id": "113e0bb9", "metadata": {"tags": ["learner", "lect_03"]}, "source": ["<h3>Implementing NRE</h3>\n", "\n", "Start by creating some training data. We will uniformly sample parameters like we did in the last section. Thene we will generate toy data using these specificied samples and poisson fluctuating them. Our resuintg samples, will be the data from each bin, lets print out a few on the way.\n"]}, {"cell_type": "code", "execution_count": null, "id": "f2ec8a3f", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell01\n", "\n", "n_train = 50_000\n", "\n", "# Simulate training data\n", "theta_samples = np.random.uniform(low=[0, 0], high=[200, 1], size=(n_train, 2))  # Parameter proposal\n", "x_samples = np.array([bump_simulator(theta, y) for theta in tqdm(theta_samples)])\n", "print(\"here is one set of random data poitns:\",x_samples[0])\n", "\n", "# Convert to torch tensors\n", "theta_samples = torch.tensor(theta_samples, dtype=torch.float32)\n", "x_samples = torch.tensor(x_samples, dtype=torch.float32)\n", "\n", "# Normalize the data to make the NN understand these guys\n", "x_mean = x_samples.mean(dim=0)\n", "x_std = x_samples.std(dim=0)\n", "x_samples = (x_samples - x_mean) / x_std\n", "\n", "theta_mean = theta_samples.mean(dim=0)\n", "theta_std = theta_samples.std(dim=0)\n", "theta_samples = (theta_samples - theta_mean) / theta_std"]}, {"cell_type": "markdown", "id": "976c00f1", "metadata": {"tags": ["learner", "lect_03"]}, "source": ["As our parameterized classifier, we will use a simple MLP (Multilayer Perceptron) neural network."]}, {"cell_type": "code", "execution_count": null, "id": "6eb0e0e4", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell02\n", "\n", "def build_mlp(input_dim, hidden_dim, output_dim, layers, activation=nn.GELU()):\n", "    \"\"\"Create an MLP from the configuration.\"\"\"\n", "    seq = [nn.Linear(input_dim, hidden_dim), activation]\n", "    for _ in range(layers):\n", "        seq += [nn.Linear(hidden_dim, hidden_dim), activation]\n", "    seq += [nn.Linear(hidden_dim, output_dim)]\n", "    return nn.Sequential(*seq)"]}, {"cell_type": "markdown", "id": "2f79dc7f", "metadata": {"tags": ["learner", "lect_03"]}, "source": ["Now we will create a neural ratio estimator class, with a corresponding loss function. The goal is to estimate the likelihood ratio between two distributions using a neural network. What we will want to do then is define a good paring\n", "\n", " * parameters $\\theta_i$ correspond to the correct distribution $p_i(x)$\n", "\n", "and a bad pairing\n", "\n", " * parameters $\\theta_j$ correspond to the an incorrect distribution $p_i(x)$, which we define by taking our sampled histogram $p_i(x)$ and randomly pointing it to a different set of parameters $\\theta_j$ such that $\\theta_j\\neq\\theta_{i}$, ie the parameters $\\theta_j$ cannot be used to generate the correct histogram\n", "\n", "We can also just phrase this by noting that in order to discriminate between samples that are drawn from a joint distribution $\\{x, \\theta\\} \\sim p(x\\mid\\theta)$ and those from a product of marginals $\\{x, \\theta\\} \\sim p(x)\\,p(\\theta)$. In the first $p(x)$ is conditioned on $\\theta$, in the later there is no defined conidtioning.\n", "\n", "By setting up this pairing, we discriminate the correct pairing of a histogram $p(x)$ to its underlying fit parameter $\\theta$.\n", "\n", "For the training, to obtain samples from the joint distribution, we draw pairs $(x, \\theta)$ from the true model, where $x$ is generated based on $\\theta$. To obtain samples from incorrect pairing (ie the product of marginals) we shuffle the joint samples within a batch, breaking the conditioning between $x$ on $\\theta$.\n", "\n", "The binary cross-entropy (a common loss function) is used as the classifier loss to distinguish samples from the correct conditioning to the incorrect,\n", "\n", "$$ L = - \\sum_i y_i \\log(p_i)$$\n", "\n", "where $y_i$ are the true labels and $p_i$ the softmaxxed probabilities (i.e, values between 0 and 1).\n", "\n", "Lets spell out our loss very clearly below."]}, {"cell_type": "code", "execution_count": null, "id": "83acd24a", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell03\n", "\n", "class NeuralRatioEstimator(pl.LightningModule):\n", "    \"\"\" Simple neural likelihood-to-evidence ratio estimator, using an MLP as a parameterized classifier.\n", "    \"\"\"\n", "    def __init__(self, x_dim, theta_dim):\n", "        super().__init__()\n", "        self.classifier = build_mlp(input_dim=x_dim + theta_dim, hidden_dim=128, output_dim=1, layers=4)\n", "\n", "    def forward(self, x):\n", "        return self.classifier(x)\n", "\n", "    def loss(self, x, theta):\n", "        #x is our input histogram, the first thing we do is clone it using repeat_interleave\n", "\n", "        # Repeat x in groups of 2 along batch axis\n", "        x = x.repeat_interleave(2, dim=0)\n", "\n", "        #Now we need to make a random permutation of the theta parameters to make an incorrect pairing\n", "        # Get a shuffled version of theta\n", "        theta_shuffled = theta[torch.randperm(theta.shape[0])]\n", "\n", "        # We insert this into our new theta array to get the correct an incorrect pairings\n", "        # Interleave theta and shuffled theta\n", "        theta = torch.stack([theta, theta_shuffled], dim=1).reshape(-1, theta.shape[1])\n", "\n", "        #Now we can define what is correct, and what is not labels[1::2]=0.0, will take [1,1,1,...]=>[1,0,1,0,..]\n", "        #(ie first is correct and 2nd is incorrect and so on)\n", "        # Get labels; ones for pairs from joint, zeros for pairs from marginals\n", "        labels = torch.ones(x.shape[0], device=x.device)\n", "        labels[1::2] = 0.0\n", "\n", "        # Pass through parameterized classifier to get logits\n", "        # now merge the inputs to x and theta into one vecotr and apply the mlp (note self function does this)\n", "        logits = self(torch.cat([x, theta], dim=1))\n", "        probs = torch.sigmoid(logits).squeeze()\n", "\n", "        #Finally binary cross entropy with our truth\n", "        return nn.BCELoss(reduction='none')(probs, labels)\n", "\n", "\n", "    def training_step(self, batch, batch_idx):\n", "        x, theta = batch\n", "        loss = self.loss(x, theta).mean()\n", "        self.log(\"train_loss\", loss)\n", "        return loss\n", "\n", "    def validation_step(self, batch, batch_idx):\n", "        x, theta = batch\n", "        loss = self.loss(x, theta).mean()\n", "        self.log(\"val_loss\", loss)\n", "        return loss\n", "\n", "    def configure_optimizers(self):\n", "        return torch.optim.Adam(self.parameters(), lr=3e-4)"]}, {"cell_type": "code", "execution_count": null, "id": "806b7557", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell04\n", "\n", "# Lets test out the above code, by taking 64 random histograms (x_samples[:64]) and computing the loss of the MLP\n", "# Evaluate loss; initially it should be around -log(0.5) = 0.693\n", "nre = NeuralRatioEstimator(x_dim=50, theta_dim=2)\n", "nre.loss(x_samples[:64], theta_samples[:64])"]}, {"cell_type": "markdown", "id": "d5b50877", "metadata": {"tags": ["learner", "lect_03"]}, "source": ["With the above NRE, we are now in stage to start training our NN> We can instantiate dataloader and train. Note that the training performed by `L24.3-runcell06` takes a bit of time. Note, we are using Pytorch lighting in this lesson to make the code sleek and fast."]}, {"cell_type": "code", "execution_count": null, "id": "cfb97306", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell05\n", "\n", "val_fraction = 0.1\n", "batch_size = 128\n", "n_samples_val = int(val_fraction * len(x_samples))\n", "\n", "dataset = TensorDataset(x_samples, theta_samples)\n", "\n", "dataset_train, dataset_val = random_split(dataset, [len(x_samples) - n_samples_val, n_samples_val])\n", "train_loader = DataLoader(dataset_train, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=True)\n", "val_loader = DataLoader(dataset_val, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=False)\n"]}, {"cell_type": "code", "execution_count": null, "id": "f63ff4ec", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell06\n", "\n", "trainer = pl.Trainer(max_epochs=12)\n", "trainer.fit(model=nre, train_dataloaders=train_loader, val_dataloaders=val_loader);"]}, {"cell_type": "markdown", "id": "d7ee0134", "metadata": {"tags": ["learner", "lect_03"]}, "source": ["The classifier will now take in a histogram and the two theta parameters and give you an approximate likehlihood that the parameters $\\theta_i$ are a better fit for $x$. In otherwords logits are now an estimator for the likelihood ratio. We can write down a log-likelihood function and use it to sample from the corresponding posterior distribution, just like before. The reason we can do this is when we run MCMC we are taking a ratio of likelihoods. We now replace that likelihood ratio with the NN approximation\n", "\n", "$$\n", "r = \\frac{L(\\theta_i|x)}{L(\\theta_j|x)} \\approx \\frac{NN(x,\\theta_i)}{NN(x,\\theta_j)}\n", "$$"]}, {"cell_type": "code", "execution_count": null, "id": "46420b3b", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell07\n", "\n", "def log_like(theta, x):\n", "    \"\"\" Log-likelihood ratio estimator using trained classifier logits.\n", "    \"\"\"\n", "    #convert to torch and normalize inputs\n", "    x = torch.Tensor(x)\n", "    theta = torch.Tensor(theta)\n", "\n", "    # Normalize\n", "    x = (x - x_mean) / x_std\n", "    theta = (theta - theta_mean) / theta_std\n", "    #Ensures array input is correct\n", "    x = torch.atleast_1d(x)\n", "    theta = torch.atleast_1d(theta)\n", "    #now apply the NN\n", "    return nre.classifier(torch.cat([x, theta], dim=-1)).squeeze().detach().numpy()\n", "\n", "theta_test = np.array([90, 0.8])\n", "x_test = bump_simulator(theta_test, y)\n", "\n", "log_like(theta_test, x_test)"]}, {"cell_type": "markdown", "id": "ac2ac0e5", "metadata": {"tags": ["learner", "lect_03"]}, "source": ["Naturally, we need to add priors to our MCMC, if we have them. Here we use our old priors."]}, {"cell_type": "code", "execution_count": null, "id": "377b0ce5", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell08\n", "\n", "def log_post(theta, x):\n", "    \"\"\" Log-posterior distribution, for sampling.\n", "    \"\"\"\n", "    lp = log_prior(theta)\n", "    if not np.isfinite(lp):\n", "        return -np.inf\n", "    else:\n", "        return lp + log_like(theta, x)"]}, {"cell_type": "markdown", "id": "8de4fbfa", "metadata": {"tags": ["learner", "lect_03"]}, "source": ["Finally, we just use our NN with a nomral MCMC sampler, in this case, we sample with `emcee`:"]}, {"cell_type": "code", "execution_count": null, "id": "180d9bfc", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell09\n", "\n", "ndim, nwalkers = 2, 32\n", "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_post, args=(x_test,))\n", "\n", "pos = opt.x + 1e-3 * np.random.randn(nwalkers, ndim)\n", "sampler.run_mcmc(pos, 2000, progress=True);"]}, {"cell_type": "markdown", "id": "223763cb", "metadata": {"tags": ["learner", "lect_03"]}, "source": ["Plot approximate posterior:"]}, {"cell_type": "code", "execution_count": null, "id": "fbb30eb9", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell10\n", "\n", "flat_samples_nre = sampler.get_chain(discard=1000, flat=True)\n", "corner.corner(flat_samples_nre, labels=[\"amp_s\", \"mu_s\"], truths=[90, 0.8], range=[(25, 150), (0.5, 1.)]);\n"]}, {"cell_type": "markdown", "id": "c43bc7db", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='exercises_24_3'></a>     \n", "\n", "| [Top](#section_24_0) | [Restart Section](#section_24_3) | [Next Section](#section_24_4) |\n"]}, {"cell_type": "markdown", "id": "5aff9d51", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 24.3.1</span>\n", "\n", "Complete the code below to compare the NRE approach with the previous explicit likelihood (EL) approach from section 1. How do the corner plots compare? Is one better?\n", "\n", "A) The results are comparable with no clear winner.\n", "\n", "B) The results are comparable, but the NRE method is better (producing a narrower range of results).\n", "\n", "C) The results are comparable, but the EL method is better (producing a narrower range of results).\n", "\n", "\n", "<br>"]}, {"cell_type": "code", "execution_count": null, "id": "b7447a3b", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L24.3.1\n", "\n", "#First create a figure with the result from `flat_samples_nre` above\n", "fig=corner.corner(flat_samples_nre, labels=[\"amp_s\", \"mu_s\"], truths=[90, 0.8], range=[(60, 120), (0.75, 0.85)]);\n", "\n", "def log_like_sig_old(params, y, x):\n", "    #Log-likehood function for a Gaussian bump (amp_s, mu_s) on top of a fixed PL background.\n", "\n", "    amp_s, mu_s = params\n", "    std_s, amp_b, exp_b = 0.05, 50, -0.5\n", "    mu = bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b)\n", "    return poisson.logpmf(x, mu).sum()\n", "\n", "def log_post_old(thetas, y, x):\n", "    #Log-posterior function for a Gaussian bump (amp_s, mu_s) on top of a fixed PL background.\n", "    \n", "    lp = log_prior(thetas)\n", "    if not np.isfinite(lp):\n", "        return -np.inf\n", "    else:\n", "        return lp + log_like_sig_old(thetas, y, x)\n", "\n", "    \n", "# Sampling with `emcee`\n", "## see  above, be sure to use x_test for the new x values and  log_post_old\n", "#YOUR CODE HERE\n", "\n", "\n", "#plot the new values from mcmc in red\n", "flat_samples_mcmc = sampler.get_chain(discard=1000, flat=True)\n", "corner.corner(flat_samples_mcmc,color='red', labels=[\"amp_s\", \"mu_s\"], truths=[90, 0.8],fig=fig, range=[(60, 120), (0.75, 0.85)]);\n"]}, {"cell_type": "markdown", "id": "b86f43e3", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 24.3.2</span>\n", "\n", "Now lets look into a situation where this approach starts to break down. Edit the variable `theta_samples_tmp` to set the amplitude range of our sample to be from 0 to 2000 (previously it was 0 to 200, defined earlier in this section). How does the fit look now?\n", "\n", "A) The fit is comparable or better.\n", "\n", "B) The fit is okay in `amp_s` but not `mu_s`.\n", "\n", "C) The fit is okay in `mu_s` but not `amp_s`.\n", "\n", "D) The fit is way off in both `amp_s` and `mu_s`.\n", "\n", "\n", "<br>"]}, {"cell_type": "code", "execution_count": null, "id": "b6c1d025", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L24.3.2\n", "\n", "n_train = 50_000\n", "\n", "# Simulate training data\n", "theta_samples_tmp = np.random.uniform(####YOUR CODE HERE####)  # Parameter proposal\n", "x_samples_tmp = np.array([bump_simulator(theta, y) for theta in tqdm(theta_samples_tmp)])\n", "\n", "# Convert to torch tensors\n", "theta_samples_tmp = torch.tensor(theta_samples_tmp, dtype=torch.float32)\n", "x_samples_tmp = torch.tensor(x_samples_tmp, dtype=torch.float32)\n", "\n", "# Normalize the data\n", "x_mean_tmp = x_samples_tmp.mean(dim=0)\n", "x_std_tmp = x_samples_tmp.std(dim=0)\n", "x_samples_tmp = (x_samples_tmp - x_mean_tmp) / x_std_tmp\n", "theta_mean_tmp = theta_samples_tmp.mean(dim=0)\n", "theta_std_tmp = theta_samples_tmp.std(dim=0)\n", "theta_samples_tmp = (theta_samples_tmp - theta_mean_tmp) / theta_std_tmp\n", "\n", "#training code\n", "val_fraction = 0.1\n", "batch_size = 128\n", "n_samples_val_tmp = int(val_fraction * len(x_samples_tmp))\n", "dataset_tmp = TensorDataset(x_samples_tmp, theta_samples_tmp)\n", "dataset_train_tmp, dataset_val_tmp = random_split(dataset_tmp, [len(x_samples_tmp) - n_samples_val_tmp, n_samples_val_tmp])\n", "train_loader_tmp = DataLoader(dataset_train_tmp, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=True)\n", "val_loader_tmp = DataLoader(dataset_val_tmp, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=False)#\n", "\n", "nre_largesample = NeuralRatioEstimator(x_dim=50, theta_dim=2)\n", "trainer = pl.Trainer(max_epochs=12)\n", "trainer.fit(model=nre_largesample, train_dataloaders=train_loader_tmp, val_dataloaders=val_loader_tmp);\n", "\n", "def log_like(theta, x):\n", "    \"\"\" Log-likelihood ratio estimator using trained classifier logits.\n", "    \"\"\"\n", "\n", "    x = torch.Tensor(x)\n", "    theta = torch.Tensor(theta)\n", "\n", "    # Normalize\n", "    x = (x - x_mean) / x_std\n", "    theta = (theta - theta_mean) / theta_std\n", "\n", "    x = torch.atleast_1d(x)\n", "    theta = torch.atleast_1d(theta)\n", "\n", "    return nre_largesample.classifier(torch.cat([x, theta], dim=-1)).squeeze().detach().numpy()\n", "\n", "def log_post(theta, x):\n", "    \"\"\" Log-posterior distribution, for sampling.\n", "    \"\"\"\n", "    lp = log_prior(theta)\n", "    if not np.isfinite(lp):\n", "        return -np.inf\n", "    else:\n", "        return lp + log_like(theta, x)\n", "\n", "ndim, nwalkers = 2, 32\n", "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_post, args=(x_test,))\n", "\n", "pos = opt.x + 1e-3 * np.random.randn(nwalkers, ndim)\n", "sampler.run_mcmc(pos, 2000, progress=True);\n", "\n", "flat_samples_nre2 = sampler.get_chain(discard=1000, flat=True)\n", "fig=corner.corner(flat_samples_nre2, labels=[\"amp_s\", \"mu_s\"], truths=[90, 0.8], range=[(60, 120), (0.75, 0.85)]);\n"]}, {"cell_type": "markdown", "id": "72c70b9f", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_24_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L24.4 Implicit Likelihood Method 3: Neural Posterior Estimation</h2>  \n", "\n", "| [Top](#section_24_0) | [Previous Section](#section_24_3) | [Exercises](#exercises_24_4) | [Next Section](#section_24_5) |\n"]}, {"cell_type": "markdown", "id": "bc53ad16", "metadata": {"tags": ["learner", "lect_04"]}, "source": ["<h3>Neural posterior estimation (NPE)</h3>\n", "\n", "Neural Posterior Estimation (NPE) is another method for directly modeling the posterior distribution $p(\\theta \\mid x)$ without needing to go through the likelihood function. This is achieved using a generative model, specifically a **normalizing flow** (generative model that transforms a simple distribution, like a Gaussian, into a more complex one through a series of invertible transformations), which is trained to produce the posterior distribution directly.\n", "\n", "Unlike Approximate Bayesian Computation (ABC), NPE does not require setting an epsilon threshold, as it directly models the posterior density. For high-dimensional data, NPE can incorporate a data compression step using a neural network to condense the data into a lower-dimensional vector, which is then used by the normalizing flow. Another advantage is that this method simultaneously performs posterior estimation and learns data compression, making it efficient for handling complex data distributions."]}, {"cell_type": "markdown", "id": "2cf044d9", "metadata": {"tags": ["learner", "lect_04"]}, "source": ["<p align=\"center\">\n", "<img alt=\"neural posterior estimation\" src=\"https://raw.githubusercontent.com/mitx-8s50/images/main/L24/npe.png\" width=\"800\"/>\n", "</p>"]}, {"cell_type": "markdown", "id": "9d2756a4", "metadata": {"tags": ["learner", "lect_04"]}, "source": ["<h3>Implementing NPE</h3>\n", "\n", "Below we implement the NPE. First, we create a function to construct and return a simple normalizing flow model. The model uses a standard normal distribution as its base distribution and applies a series of invertible transformations to create a more complex distribution. The number of transformations (layers) and the dimensions of the input, hidden features, and context features are configurable through the function's parameters."]}, {"cell_type": "code", "execution_count": null, "id": "543f5af9", "metadata": {"tags": ["learner", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L24.4-runcell01\n", "\n", "from nflows.flows.base import Flow\n", "from nflows.distributions.normal import StandardNormal\n", "from nflows.transforms.base import CompositeTransform\n", "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform\n", "from nflows.transforms.permutations import ReversePermutation"]}, {"cell_type": "code", "execution_count": null, "id": "150d56ee", "metadata": {"tags": ["learner", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L24.4-runcell02\n", "\n", "def get_flow(d_in=2, d_hidden=32, d_context=16, n_layers=4):\n", "    \"\"\" Instantiate a simple (Masked Autoregressive) normalizing flow.\n", "    \"\"\"\n", "\n", "    base_dist = StandardNormal(shape=[d_in])\n", "\n", "    transforms = []\n", "    for _ in range(n_layers):\n", "        transforms.append(ReversePermutation(features=d_in))\n", "        transforms.append(MaskedAffineAutoregressiveTransform(features=d_in, hidden_features=d_hidden, context_features=d_context))\n", "    transform = CompositeTransform(transforms)\n", "\n", "    flow = Flow(transform, base_dist)\n", "    return flow\n", "\n", "# Instantiate flow\n", "flow = get_flow()\n", "\n", "# Make sure sampling and log-prob calculation makes sense\n", "samples, log_prob = flow.sample_and_log_prob(num_samples=100, context=torch.randn(2, 16))\n", "print(samples.shape, log_prob.shape)"]}, {"cell_type": "markdown", "id": "7b6fca58", "metadata": {"tags": ["learner", "lect_04"]}, "source": ["Now we construct the neural posterior estimator. It uses a normalizing flow as a (conditional) posterior density estimator, and a feature-extraction network (this is just another NN) that aligns the directions of variations in parameters $\\theta$ and data $x$.\n", "\n", "$$   L = -\\log p_\\phi(\\theta\\mid s_\\varphi(x))$$\n", "\n", "where $\\{\\phi, \\varphi\\}$ are the parameters of the normalizing flow and featurizer MLP, respectively.\n", "\n", "Ultimately, the goal of training this neural posterior estimator is to adjust the parameters $\\{\\phi, \\varphi\\}$ of both the normalizing flow and the feature-extraction network so that the model accurately estimates the posterior distribution $p(\\theta\u2223x)$.\n", "\n", "In particular:\n", "- the normalizing flow (with parameters $\\phi$) learns to model the posterior distribution of $\\theta$ conditioned on the features $s_{\\varphi}(x)$.\n", "- the feature-extraction network (with parameters $\\varphi$) learns to transform the raw data $x$ into a feature space that makes it easier for the normalizing flow to estimate the posterior distribution."]}, {"cell_type": "code", "execution_count": null, "id": "e7ecdb56", "metadata": {"tags": ["learner", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L24.4-runcell03\n", "\n", "class NeuralPosteriorEstimator(pl.LightningModule):\n", "    \"\"\" Simple neural posterior estimator class using a normalizing flow as the posterior density estimator.\n", "    \"\"\"\n", "    def __init__(self, featurizer, d_context=16,d_hidden=32):\n", "        super().__init__()\n", "        self.featurizer = featurizer\n", "        self.flow = get_flow(d_in=2, d_hidden=d_hidden, d_context=d_context, n_layers=4)\n", "\n", "    def forward(self, x):\n", "        return self.featurizer(x)\n", "    \n", "    def loss(self, x, theta):\n", "        context = self(x)\n", "        return -self.flow.log_prob(inputs=theta, context=context)\n", "\n", "    def training_step(self, batch, batch_idx):\n", "        x, theta = batch\n", "        loss = self.loss(x, theta).mean()\n", "        self.log(\"train_loss\", loss)\n", "        return loss\n", "\n", "    def validation_step(self, batch, batch_idx):\n", "        x, theta = batch\n", "        loss = self.loss(x, theta).mean()\n", "        self.log(\"val_loss\", loss)\n", "        return loss\n", "\n", "    def configure_optimizers(self):\n", "        return torch.optim.Adam(self.parameters(), lr=3e-4)"]}, {"cell_type": "markdown", "id": "cf89c08c", "metadata": {"tags": ["learner", "lect_04"]}, "source": ["Next we instantiate the NPE class and look at the loss."]}, {"cell_type": "code", "execution_count": null, "id": "9ffb4176", "metadata": {"tags": ["learner", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L24.4-runcell04\n", "\n", "npe = NeuralPosteriorEstimator(featurizer=build_mlp(input_dim=50, hidden_dim=128, output_dim=16, layers=4))\n", "npe.loss(x_samples[:64], theta_samples[:64])"]}, {"cell_type": "markdown", "id": "efa87653", "metadata": {"tags": ["learner", "lect_04"]}, "source": ["Now we train using the same data as before (and, again, this takes some time)."]}, {"cell_type": "code", "execution_count": null, "id": "979538a5", "metadata": {"tags": ["learner", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L24.4-runcell05\n", "\n", "trainer = pl.Trainer(max_epochs=12)\n", "trainer.fit(model=npe, train_dataloaders=train_loader, val_dataloaders=val_loader);"]}, {"cell_type": "markdown", "id": "43fcf1aa", "metadata": {"tags": ["learner", "lect_04"]}, "source": ["Get a test data sample, pass it through the feature extractor, and condition the flow density estimator on it. We get posterior samples by drawing from \n", "$$\\theta \\sim p_\\phi(\\theta\\mid s_\\varphi(x)).$$"]}, {"cell_type": "code", "execution_count": null, "id": "42923c77", "metadata": {"tags": ["learner", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L24.4-runcell06\n", "\n", "theta_test = np.array([90, 0.8])\n", "x_test = bump_simulator(theta_test, y)"]}, {"cell_type": "code", "execution_count": null, "id": "8c97ea6e", "metadata": {"tags": ["learner", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L24.4-runcell07\n", "\n", "x_test_norm = (torch.Tensor(x_test) - x_mean) / x_std\n", "context = npe.featurizer(x_test_norm).unsqueeze(0)"]}, {"cell_type": "markdown", "id": "1a788ffb", "metadata": {"tags": ["learner", "lect_04"]}, "source": ["Here, the probability distributions show again that the amplitude is incorrect by about 5-10% but the mean is very close to the truth value. Note the greatly expanded scale for the mean."]}, {"cell_type": "code", "execution_count": null, "id": "34934952", "metadata": {"tags": ["learner", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L24.4-runcell08\n", "\n", "samples_test_npe = npe.flow.sample(num_samples=2000, context=context) * theta_std + theta_mean\n", "samples_test_npe = samples_test_npe.detach().numpy()\n", "corner.corner(samples_test_npe, labels=[\"amp_s\", \"mu_s\"], truths=[90, 0.8]);"]}, {"cell_type": "markdown", "id": "865b16c9", "metadata": {"tags": ["learner", "lect_04"]}, "source": ["To summarize, with NPE there are 2 networks we are defining: the flow and the context network. The featurizer is an MLP for the context network, and the flow uses the featurizer to embed in a smaller dimensional space."]}, {"cell_type": "markdown", "id": "d3747380", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='exercises_24_4'></a>     \n", "\n", "| [Top](#section_24_0) | [Restart Section](#section_24_4) | [Next Section](#section_24_5) |\n"]}, {"cell_type": "markdown", "id": "b4e479ae", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 24.4.1</span>\n", "\n", "In both neural ratio and neural posterior estimation, what role does the feature extractor (which was an MLP in the code above) play? Select ALL that apply.\n", "\n", "A) Extract features that are informative of the parameters of interest\\\n", "B) Increase the complexity of the model to improve performance\\\n", "C) Normalize the data to a common scale\\\n", "D) Remove noise from the data to improve estimation accuracy\\\n", "E) Reduce the dimensionality of the data\n", "\n", "\n", "<br>"]}, {"cell_type": "markdown", "id": "27e151bf", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 24.4.2</span>\n", "\n", "Let's check the limitations of this tool as an estimator. Run the code below, where we shrink the number of parameters in the MLP used for the normalizing flow by an order of magnitude, then plot the results in red alongside the results from `L24.4-runcell08` above. How does this perform?\n", "\n", "A) This performs better than the previous NPE.\n", "\n", "B) This performs just as well as the previous NPE.\n", "\n", "C) This performs more poorly than the previous NPE.\n", "\n", "\n", "<br>"]}, {"cell_type": "code", "execution_count": null, "id": "ea8688a7", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L24.4.2\n", "\n", "#first create a corner plot with the results from `L24.4-runcell08`\n", "fig = corner.corner(samples_test_npe, labels=[\"amp_s\", \"mu_s\"], truths=[90, 0.8]);\n", "\n", "#now define a new NPE with fewer parameters\n", "npe_shrink = NeuralPosteriorEstimator(d_context=2,d_hidden=4,featurizer=build_mlp(input_dim=50, hidden_dim=2, output_dim=2, layers=1))\n", "\n", "\n", "npe_shrink.loss(x_samples[:64], theta_samples[:64])\n", "trainer = pl.Trainer(max_epochs=12)\n", "trainer.fit(model=npe_shrink, train_dataloaders=train_loader, val_dataloaders=val_loader);\n", "\n", "context_shrink = npe_shrink.featurizer(x_test_norm).unsqueeze(0)\n", "samples_test_shrink = npe_shrink.flow.sample(num_samples=2000, context=context_shrink) * theta_std + theta_mean\n", "samples_test_shrink = samples_test_shrink.detach().numpy()[0]\n", "\n", "#plot the new results in red\n", "corner.corner(samples_test_shrink, color='red', fig=fig, labels=[\"amp_s\", \"mu_s\"], truths=[90, 0.8]);"]}, {"cell_type": "markdown", "id": "62bd03d6", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_24_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L24.5 A more complicated example: distribution of point sources in a 2D image</h2>  \n", "\n", "| [Top](#section_24_0) | [Previous Section](#section_24_4) | [Exercises](#exercises_24_5) | [Next Section](#section_24_6) |\n"]}, {"cell_type": "markdown", "id": "67b59411", "metadata": {"tags": ["learner"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.3x+3T2023/block-v1:MITxT+8.S50.3x+3T2023+type@sequential+block@seq_LS24/block-v1:MITxT+8.S50.3x+3T2023+type@vertical+block@vert_LS24_vid3\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "3f6c0386", "metadata": {"tags": ["learner", "lect_05"]}, "source": ["<h3>Overview</h3>\n", "\n", "Finally, let's look at a more complicated example, one that is closer to a typical application of SBI and where the likelihood is formally intractable.\n", "\n", "The forward model simulates a map of point sources with mean counts drawn from a power law (Pareto) distribution. The distribution of the mean counts is given by the following equation:\n", "\n", "$$\n", "\\frac{\\mathrm dn}{\\mathrm  ds} = A s^{\\beta}\n", "$$\n", "\n", "where $A$ is the amplitude (`amp_b`), $s$ is the flux, and $\\beta$ is the exponent (`exp_b`). The fluxes are drawn from a truncated power law with minimum and maximum bounds, $s_\\text{min}$ and $s_\\text{max}$, respectively. This needs to be done because an unbounded power law distribution does not have a finite integral and, therefore, cannot represent a physical probability distribution.\n", "\n", "The number of sources is determined by integrating the power law distribution within the flux limits and taking a Poisson realization:\n", "\n", "$$\n", "N_\\text{sources} \\sim \\text{Pois}\\left(\\int_{s_\\text{min}}^{s_\\text{max}} \\, \\mathrm ds \\frac{\\mathrm dn}{\\mathrm ds}\\right)\n", "$$\n", "\n", "For each source, a position is randomly assigned within the box of size `box_size`. The fluxes are then binned into a grid with `resolution` number of bins in both x and y directions. The resulting map is convolved with a Gaussian point spread function (PSF) with a standard deviation of `sigma_psf` to account for the spatial resolution of the instrument.\n", "\n", "The output is a 2D map of counts representing the simulated observation of the point sources in the sky."]}, {"cell_type": "code", "execution_count": null, "id": "b9a7c978", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell01\n", "\n", "from scipy.stats import binned_statistic_2d\n", "from astropy.convolution import convolve, Gaussian2DKernel\n", "\n", "\n", "def simulate_sources(amp_b, exp_b, s_min=0.5, s_max=50.0, box_size=1., resolution=64, sigma_psf=0.01):\n", "    \"\"\" Simulate a map of point sources with mean counts drawn from a power law (Pareto) distribution dn/ds = amp_b * s ** exp_b\n", "    \"\"\"\n", "    # Get number of sources by analytically integrating dn/ds and taking Poisson realization\n", "    n_sources = np.random.poisson(-amp_b * (s_min ** (exp_b - 1)) / (exp_b - 1))\n", "\n", "    # Draw fluxes from truncated power law amp_b * s ** (exp_b - 1), with s_min and s_max as the bounds\n", "    fluxes = draw_powerlaw_flux(n_sources, s_min, s_max, exp_b)\n", "\n", "    positions = np.random.uniform(0., box_size, size=(n_sources, 2))\n", "    bins = np.linspace(0, box_size, resolution + 1)\n", "\n", "    pixel_size = box_size / resolution\n", "    kernel = Gaussian2DKernel(x_stddev=1.0 * sigma_psf / pixel_size)\n", "\n", "    mu_signal = binned_statistic_2d(x=positions[:, 0], y=positions[:, 1], values=fluxes, statistic='sum', bins=bins).statistic\n", "    counts = np.random.poisson(convolve(mu_signal, kernel))\n", "                \n", "    return fluxes, counts\n", "\n", "def draw_powerlaw_flux(n_sources, s_min, s_max, exp_b):\n", "    \"\"\"\n", "    Draw from a power law with slope `exp_b` and min/max mean counts `s_min` and `s_max`. From:\n", "    https://stackoverflow.com/questions/31114330/python-generating-random-numbers-from-a-power-law-distribution\n", "    \"\"\"\n", "    u = np.random.uniform(0, 1, size=n_sources)\n", "    s_low_u, s_high_u = s_min ** (exp_b + 1), s_max ** (exp_b + 1)\n", "    return (s_low_u + (s_high_u - s_low_u) * u) ** (1.0 / (exp_b + 1.0))\n", "\n", "fluxes, counts = simulate_sources(amp_b=200., exp_b=-1.2)\n", "plt.imshow(counts, cmap='viridis', vmax=20)\n", "plt.xlabel(\"Pixels\")\n", "plt.ylabel(\"Pixels\")"]}, {"cell_type": "markdown", "id": "05a27e18", "metadata": {"tags": ["learner", "lect_05"]}, "source": ["Here we plot many different simulations, varying the parameters `amp_b` and `exp_b`. Notice that when `exp_b` is small, and the power law is flatter, there are many sources of different brightness; whereas when the power law is steep, there are many more dim sources.\n", "\n", "Our goal will be to infer the parameters, given some image (observation) like the ones below."]}, {"cell_type": "code", "execution_count": null, "id": "cfeb8118", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell02\n", "\n", "# Draw parameters from the prior\n", "n_params = 16\n", "\n", "amp_b_prior = (100., 300.)\n", "exp_b_prior = (-2.0, -0.5)\n", "\n", "amp_bs = np.random.uniform(amp_b_prior[0], amp_b_prior[1], n_params)\n", "exp_bs = np.random.uniform(exp_b_prior[0], exp_b_prior[1], n_params)\n", "\n", "# Plot the data samples on a grid\n", "fig, axes = plt.subplots(4, 4, figsize=(14, 14))\n", "\n", "for i, ax in enumerate(axes.flatten()):\n", "    fluxes, counts = simulate_sources(amp_b=amp_bs[i], exp_b=exp_bs[i])\n", "    im = ax.imshow(counts, cmap='viridis', vmax=20)\n", "    ax.set_title(f'$A_b={amp_bs[i]:.2f}, n_b={exp_bs[i]:.2f}$')\n", "    ax.set_xticks([])\n", "    ax.set_yticks([])\n", "\n", "plt.show()"]}, {"cell_type": "markdown", "id": "0ef1ea03", "metadata": {"tags": ["learner", "lect_05"]}, "source": ["<h3>Explicit likelihood</h3>\n", "\n", "The (marginal) likelihood, which we would need to plug into something like MCMC, is computationally intractable! This is because it involves an integral over a cumbersome latent space, which consists of all possible number $n$ of sources and their positions $\\{z\\}=\\{x, y\\}_{i=1}^{n}$. Let's write this out formally:\n", "$$p(x \\mid \\theta)=\\sum_{n} \\int \\mathrm{d}^{n} \\{z\\}\\, p\\left(n \\mid \\theta\\right) \\prod_i^{n} p\\left(z_{i} \\mid \\theta\\right) \\, p\\left(x \\mid \\theta,\\left\\{z_{i}\\right\\}\\right)$$\n", "\n", "We'll have to use simulation based inference!\n"]}, {"cell_type": "markdown", "id": "74eab30c", "metadata": {"tags": ["learner", "lect_05"]}, "source": ["<h3>Implicit inference: Neural posterior estimation</h3>\n", "\n", "Lacking the option for using a computational likelihood, let's instead use neural posterior estimation with a normalizing flow again. Get a training sample:"]}, {"cell_type": "code", "execution_count": null, "id": "ee9d98d3", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell03\n", "\n", "n_train = 20_000\n", "\n", "# Sample from prior, then simulate\n", "theta_samples = np.random.uniform(low=[10., -3.], high=[200., -0.99], size=(n_train, 2))\n", "x_samples = np.array([simulate_sources(theta[0], theta[1])[1] for theta in tqdm(theta_samples)])\n", "\n", "# Convert to torch tensors\n", "theta_samples = torch.Tensor(theta_samples)\n", "x_samples = torch.Tensor(x_samples)\n", "\n", "# Normalize the data\n", "x_mean = x_samples.mean(dim=0)\n", "x_std = x_samples.std(dim=0)\n", "x_samples = (x_samples - x_mean) / x_std\n", "\n", "theta_mean = theta_samples.mean(dim=0)\n", "theta_std = theta_samples.std(dim=0)\n", "theta_samples = (theta_samples - theta_mean) / theta_std\n"]}, {"cell_type": "code", "execution_count": null, "id": "38814878", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell04\n", "\n", "val_fraction = 0.1\n", "batch_size = 64\n", "n_samples_val = int(val_fraction * len(x_samples))\n", "\n", "dataset = TensorDataset(x_samples, theta_samples)\n", "\n", "dataset_train, dataset_val = random_split(dataset, [len(x_samples) - n_samples_val, n_samples_val])\n", "train_loader = DataLoader(dataset_train, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=True)\n", "val_loader = DataLoader(dataset_val, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=False)\n"]}, {"cell_type": "markdown", "id": "890e889f", "metadata": {"tags": ["learner", "lect_05"]}, "source": ["When dealing with high-dimensional data, such as images like we have here, you can't directly input it into a normalizing flow, which requires a 1D data vector. To address this, a convolutional neural network (CNN) is used as a feature extractor to process the image and output a 1D summary. This summary is then fed into the normalizing flow (i.e., the normalizing flow will be conditioned on the output of the CNN) keeping the overall process the same but adapting it for 2D image data.\n", "\n", "Define the CNN below (in addition to some functions defined in previous sections that we will need again):"]}, {"cell_type": "code", "execution_count": null, "id": "e30772a3", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell05\n", "\n", "class CNN(nn.Module):\n", "    \"\"\" Simple CNN feature extractor.\n", "    \"\"\"\n", "    def __init__(self, output_dim):\n", "        super(CNN, self).__init__()\n", "\n", "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n", "        self.pool1 = nn.MaxPool2d(2, 2)\n", "\n", "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n", "        self.pool2 = nn.MaxPool2d(2, 2)\n", "\n", "        self.fc1 = nn.Linear(16 * 16 * 16, 64)\n", "        self.fc2 = nn.Linear(64, output_dim)\n", "\n", "    def forward(self, x):\n", "        \n", "        x = x.unsqueeze(1)  # Add channel dim\n", "        \n", "        x = self.pool1(F.leaky_relu(self.conv1(x), negative_slope=0.02))\n", "        x = self.pool2(F.leaky_relu(self.conv2(x), negative_slope=0.02))\n", "\n", "        x = x.view(x.size(0), -1)\n", "\n", "        x = F.leaky_relu(self.fc1(x), negative_slope=0.01)\n", "        x = self.fc2(x)\n", "\n", "        return x\n", "\n", "\n", "#functions defined in previous sections, redefined here\n", "from nflows.flows.base import Flow\n", "from nflows.distributions.normal import StandardNormal\n", "from nflows.transforms.base import CompositeTransform\n", "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform\n", "from nflows.transforms.permutations import ReversePermutation\n", "\n", "def get_flow(d_in=2, d_hidden=32, d_context=16, n_layers=4):\n", "    \"\"\" Instantiate a simple (Masked Autoregressive) normalizing flow.\n", "    \"\"\"\n", "\n", "    base_dist = StandardNormal(shape=[d_in])\n", "\n", "    transforms = []\n", "    for _ in range(n_layers):\n", "        transforms.append(ReversePermutation(features=d_in))\n", "        transforms.append(MaskedAffineAutoregressiveTransform(features=d_in, hidden_features=d_hidden, context_features=d_context))\n", "    transform = CompositeTransform(transforms)\n", "\n", "    flow = Flow(transform, base_dist)\n", "    return flow\n", "\n", "\n", "class NeuralPosteriorEstimator(pl.LightningModule):\n", "    \"\"\" Simple neural posterior estimator class using a normalizing flow as the posterior density estimator.\n", "    \"\"\"\n", "    def __init__(self, featurizer, d_context=16,d_hidden=32):\n", "        super().__init__()\n", "        self.featurizer = featurizer\n", "        self.flow = get_flow(d_in=2, d_hidden=d_hidden, d_context=d_context, n_layers=4)\n", "\n", "    def forward(self, x):\n", "        return self.featurizer(x)\n", "    \n", "    def loss(self, x, theta):\n", "        context = self(x)\n", "        return -self.flow.log_prob(inputs=theta, context=context)\n", "\n", "    def training_step(self, batch, batch_idx):\n", "        x, theta = batch\n", "        loss = self.loss(x, theta).mean()\n", "        self.log(\"train_loss\", loss)\n", "        return loss\n", "\n", "    def validation_step(self, batch, batch_idx):\n", "        x, theta = batch\n", "        loss = self.loss(x, theta).mean()\n", "        self.log(\"val_loss\", loss)\n", "        return loss\n", "\n", "    def configure_optimizers(self):\n", "        return torch.optim.Adam(self.parameters(), lr=3e-4)"]}, {"cell_type": "markdown", "id": "231e0404", "metadata": {"tags": ["learner", "lect_05"]}, "source": ["Now train the CNN, as we have done before."]}, {"cell_type": "code", "execution_count": null, "id": "0f5154a6", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell06\n", "\n", "npe = NeuralPosteriorEstimator(featurizer=CNN(output_dim=32), d_context=32)\n", "npe.loss(x_samples[:64], theta_samples[:64])"]}, {"cell_type": "code", "execution_count": null, "id": "c6989879", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell07\n", "\n", "trainer = pl.Trainer(max_epochs=15)\n", "trainer.fit(model=npe, train_dataloaders=train_loader, val_dataloaders=val_loader);"]}, {"cell_type": "code", "execution_count": null, "id": "a70ba905", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell08\n", "\n", "npe = npe.eval()"]}, {"cell_type": "markdown", "id": "118b2b78", "metadata": {"tags": ["learner", "lect_05"]}, "source": ["<h3>Results</h3>\n", "\n", "Now we will get a test map, extract features, condition the normalizing flow, and extract samples."]}, {"cell_type": "code", "execution_count": null, "id": "53369584", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell09\n", "\n", "params_test = np.array([15., -1.4])\n", "x_test = simulate_sources(params_test[0], params_test[1])[1]"]}, {"cell_type": "code", "execution_count": null, "id": "645150db", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell10\n", "\n", "x_test_norm = (torch.Tensor(x_test) - x_mean) / x_std\n", "context = npe.featurizer(x_test_norm.unsqueeze(0))\n", "\n", "samples_test = npe.flow.sample(num_samples=10000, context=context) * theta_std + theta_mean\n", "samples_test = samples_test.detach().numpy()\n", "\n", "corner.corner(samples_test, labels=[\"amp\", \"exp\"], truths=params_test);"]}, {"cell_type": "markdown", "id": "43e7be0e", "metadata": {"tags": ["learner", "lect_05"]}, "source": ["Ultimately, this looks kind of okay. The amplitude is fairly well-constrained, but the slope is perhaps less certain. This could be due to uninformative data, a poor feature extractor (like the CNN), insufficient training samples, or an improperly trained normalizing flow. Unlike MCMC, these methods lack asymptotic guarantees, meaning the results might look reasonable but aren't necessarily reliable."]}, {"cell_type": "markdown", "id": "2468c4d2", "metadata": {"tags": ["learner", "lect_05"]}, "source": ["<h3>Test of statistical coverage</h3>\n", "\n", "We can do some checks to make sure that our posterior has the correct statistical interpretation. <a href=\"https://arxiv.org/abs/2209.01845\" target=\"_blank\">This paper</a> offers more insight into this topic. The main goal here is you want to make sure you posterior reflects the actual Gaussian pdf that you whish it to reflect.\n", "\n", "In particular, let's test the posterior statistical coverage by evaluating how well the Highest Posterior Density (HPD) intervals capture the true parameter values.\n", "\n", "The **Highest Posterior Density (HPD)** interval is a region in the parameter space that contains the most probable values for a given credible mass (e.g., 95% or 99%). Here, the term \"credible mass\" refers to a given percentage of the entire posterior distribution. In other words, the HPD is the smallest region, or equivalently the shortest range of the various parameters, that contains the specified credible mass. This is one way of summarizing the range of a posterior probability distribution.\n", "\n", "**Nominal coverage** is the probability criterion, or the fraction of the parameter space, that the HPD interval is intended to contain. For example, if the nominal coverage is 0.95, the HPD interval should theoretically contain the true parameter value 95% of the time.\n", "\n", "**Empirical coverage** is the fraction of true parameter values that actually fall within the HPD interval, based on a set of test cases or simulations.\n", "\n", "For a perfectly calibrated posterior estimator, the empirical and nominal coverages should be close to identical for all credibility levels.\n", "\n", "Let's create a group of test samples and use them to test how well our posterior estimator is working in this particular case."]}, {"cell_type": "code", "execution_count": null, "id": "c4bb1017", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell11\n", "\n", "n_test = 200  # How many test samples to draw for coverage test\n", "\n", "# Get samples \n", "x_test = torch.Tensor([simulate_sources(params_test[0], params_test[1])[1] for _ in range(n_test)])\n", "x_test_norm = (torch.Tensor(x_test) - x_mean) / x_std\n", "\n", "# and featurize\n", "context = npe.featurizer(x_test_norm)\n", "\n", "# Get posterior for all samples together in a batch\n", "samples_test = npe.flow.sample(num_samples=1000, context=context) * theta_std + theta_mean\n", "samples_test = samples_test.detach().numpy()"]}, {"cell_type": "markdown", "id": "bcee2c30", "metadata": {"tags": ["learner", "lect_05"]}, "source": ["As a first check, find the range of percentages of the posterior distribution included for a credible mass of 20%."]}, {"cell_type": "code", "execution_count": null, "id": "3b609755", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell12\n", "\n", "def hpd(samples, credible_mass=0.95):\n", "    \"\"\"Compute highest posterior density (HPD) of array for given credible mass.\"\"\"\n", "    sorted_samples = np.sort(samples)\n", "    interval_idx_inc = int(np.floor(credible_mass * sorted_samples.shape[0]))\n", "    n_intervals = sorted_samples.shape[0] - interval_idx_inc\n", "    interval_width = np.zeros(n_intervals)\n", "    for i in range(n_intervals):\n", "        interval_width[i] = sorted_samples[i + interval_idx_inc] - sorted_samples[i]\n", "    hdi_min = sorted_samples[np.argmin(interval_width)]\n", "    hdi_max = sorted_samples[np.argmin(interval_width) + interval_idx_inc]\n", "    return hdi_min, hdi_max\n", "\n", "hpd(samples_test[0, :, 0], credible_mass=0.2)"]}, {"cell_type": "markdown", "id": "408ea993", "metadata": {"tags": ["learner", "lect_05"]}, "source": ["That looks promising. Next, we will scan the credible mass from 50-99% in steps of 1%."]}, {"cell_type": "code", "execution_count": null, "id": "d06307f0", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell13\n", "\n", "p_nominals = np.linspace(0.01, 0.99, 50)\n", "contains_true = np.zeros((2, n_test, len(p_nominals)))\n", "\n", "for i_param in range(2):\n", "    for i, sample in enumerate(samples_test[:, :, i_param]):\n", "        for j, p_nominal in enumerate(p_nominals):\n", "            hdi_min, hdi_max = hpd(sample, credible_mass=p_nominal)\n", "            if hdi_min < params_test[i_param] < hdi_max:\n", "                contains_true[i_param, i, j] = 1"]}, {"cell_type": "code", "execution_count": null, "id": "d8f9c24a", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell14\n", "\n", "# Make two plots, one for each parameter\n", "\n", "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n", "\n", "ax[0].plot(p_nominals, contains_true[0].sum(0) / n_test)\n", "ax[0].plot([0, 1], [0, 1], color=\"black\", linestyle=\"--\")\n", "ax[0].set_xlabel(\"Nominal coverage\")\n", "ax[0].set_ylabel(\"Empirical coverage\")\n", "ax[0].set_title(\"Coverage for amplitude\")\n", "\n", "ax[1].plot(p_nominals, contains_true[1].sum(0) / n_test)\n", "ax[1].plot([0, 1], [0, 1], color=\"black\", linestyle=\"--\")\n", "ax[1].set_xlabel(\"Nominal coverage\")\n", "ax[1].set_ylabel(\"Empirical coverage\")\n", "ax[1].set_title(\"Coverage for exponent\")"]}, {"cell_type": "markdown", "id": "19178885", "metadata": {"tags": ["learner", "lect_05"]}, "source": ["In the plots generated by code cell `L24.5-runcell14`, you can see how closely the actual empirical coverage found using the test samples matches the input nominal coverage. If the correspondence is not 1:1, this means that the true value of the is not within the expected range of the HPD interval."]}, {"cell_type": "markdown", "id": "585e951b", "metadata": {"tags": ["learner", "lect_05"]}, "source": ["<h3>Further Reading</h3>\n", "\n", "- <a href=\"https://arxiv.org/abs/1911.01429\" target=\"_blank\">The frontier of simulation-based inference</a>: (Cranmer, Brehmer, Louppe): Review paper\n", "- <a href=\"http://simulation-based-inference.org/\" target=\"_blank\">Simulation-based-inference.org:</a>: List of papers and resources\n", "- <a href=\"https://github.com/smsharma/awesome-neural-sbi\" target=\"_blank\">Awesome-neural-sbi</a>: List of papers and resources"]}, {"cell_type": "markdown", "id": "112f7415", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='exercises_24_5'></a>     \n", "\n", "| [Top](#section_24_0) | [Restart Section](#section_24_5) | \n"]}, {"cell_type": "markdown", "id": "83095419", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 24.5.1</span>\n", "\n", "**Which one of the following answer options are acceptable calibration results for a posterior estimator?** In other words, which *likely* result would be preferable? Here, \"Overconfident\" means that one expects the true values to appear within a certain probability interval more often than is found using test samples. This was clearly the case for the amplitude in the example studied in the related video. In contrast, \"Conservative\" means the opposite, namely that the true value is found more often than expected in the given interval. This is the case for the exponent in the video, albeit at a much smaller level.\n", "\n", "A) Overconfident\\\n", "B) Conservative\\\n", "C) Perfectly calibrated\\\n", "D) Perfectly calibrated or conservative\\\n", "E) Perfectly calibrated or overconfident\n", "\n", "<br>"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}