{"cells": [{"cell_type": "markdown", "id": "1ee373c5", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 24: Simulation-based (Likelihood-free) Inference</h1>\n"]}, {"cell_type": "markdown", "id": "41345f3a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_24_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L24.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "fce9b388", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_24_1\">L24.1 Simple bump-on-power-law example and Explicit Likelihood</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_24_1\">L24.1 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_24_2\">L24.2 Implicit Likelihood Method 1: Approximate Bayesian Computation</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_24_2\">L24.2 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_24_3\">L24.3 Implicit Likelihood Method 2: Neural Likelihood-ratio Estimation</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_24_3\">L24.3 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_24_4\">L24.4 Implicit Likelihood Method 3: Neural Posterior Estimation</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_24_4\">L24.4 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_24_5\">L24.5 A more complicated example: distribution of point sources in a 2D image</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_24_5\">L24.5 Exercises</a></td>\n", "    </tr>\n", "</table>"]}, {"cell_type": "code", "execution_count": null, "id": "d59f3ad0", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.0-runcell00\n", "\n", "!pip install --upgrade emcee corner pytorch-lightning tqdm nflows"]}, {"cell_type": "code", "execution_count": null, "id": "0f3a0154", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.0-runcell01\n", "\n", "import torch                            #https://pytorch.org/docs/stable/torch.html\n", "import torch.nn as nn                   #https://pytorch.org/docs/stable/nn.html\n", "import torch.nn.functional as F         #https://pytorch.org/docs/stable/nn.functional.html\n", "from torch.utils.data import TensorDataset, DataLoader, random_split  #https://pytorch.org/docs/stable/data.html\n", "import numpy as np                      #https://numpy.org/doc/stable/\n", "import emcee                            #https://emcee.readthedocs.io/en/stable/\n", "from scipy.stats import poisson         #https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.poisson.html\n", "from scipy.stats import chi2            #https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2.html\n", "from scipy.optimize import basinhopping #https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.basinhopping.html\n", "from tqdm import tqdm                   #https://pypi.org/project/tqdm/\n", "import matplotlib.pyplot as plt         #https://matplotlib.org/stable/api/pyplot_summary.html#module-matplotlib.pyplot\n", "import pytorch_lightning as pl          #https://lightning.ai/docs/pytorch/stable/\n", "import corner                           #https://corner.readthedocs.io/en/latest/\n"]}, {"cell_type": "code", "execution_count": null, "id": "97d0edf8", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.0-runcell02\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title"]}, {"cell_type": "markdown", "id": "e198f21b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_24_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L24.1 Simple bump-on-power-law example and Explicit Likelihood</h2>  \n", "\n", "| [Top](#section_24_0) | [Previous Section](#section_24_0) | [Exercises](#exercises_24_1) | [Next Section](#section_24_2) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "87140346", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.1-runcell01\n", "\n", "def bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b):\n", "    \"\"\" Forward model for a Gaussian bump (amp_s, mu_s, std_s) on top of a power-law background (amp_b, exp_b).\n", "    \"\"\"\n", "    x_b = amp_b * (y ** exp_b)  # Power-law background\n", "    x_s = amp_s * np.exp(-((y - mu_s) ** 2) / (2 * std_s ** 2))  # Gaussian signal\n", "\n", "    x = x_b + x_s  # Total mean signal\n", "\n", "    return x"]}, {"cell_type": "code", "execution_count": null, "id": "73923141", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.1-runcell02\n", "\n", "#instead of sqrt(N) uncertainties, get the full asymmetric uncertainty using a chi2\n", "def poisson_interval(k, alpha=0.32): \n", "    \"\"\" Uses chi2 to get the poisson interval.\n", "    \"\"\"\n", "    a = alpha\n", "    low, high = (chi2.ppf(a/2, 2*k) / 2, chi2.ppf(1-a/2, 2*k + 2) / 2)\n", "    if k == 0: \n", "        low = 0.0\n", "    return k - low, high - k\n", "\n", "y = np.linspace(0.1, 1, 50)  # Dependent variable\n", "\n", "# Mean expected counts\n", "x_mu = bump_forward_model(y, \n", "                    amp_s=50, mu_s=0.8, std_s=0.05,  # Signal params\n", "                    amp_b=50, exp_b=-0.5)  # Background params\n", "\n", "# Realized counts\n", "np.random.seed(42)\n", "x = np.random.poisson(x_mu)\n", "x_err = np.array([poisson_interval(k) for k in x.T]).T #getting asymmetric errorbars\n", "\n", "# Plot\n", "plt.plot(y, x_mu, color='k', ls='--', label=\"Mean expected counts\")\n", "plt.errorbar(y, x, yerr=x_err, fmt='o', color='k', label=\"Realized counts\")\n", "\n", "plt.xlabel(\"$y$\")\n", "plt.ylabel(\"Counts\")\n", "\n", "plt.legend()"]}, {"cell_type": "code", "execution_count": null, "id": "d509b2c2", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.1-runcell03\n", "\n", "def log_like(theta, y, x):\n", "    \"\"\" Log-likehood function for a Gaussian bump (amp_s, mu_s, std_s) on top of a power-law background (amp_b, exp_b).\n", "    \"\"\"\n", "    amp_s, mu_s, std_s, amp_b, exp_b = theta\n", "    mu = bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b)\n", "    return poisson.logpmf(x, mu).sum()"]}, {"cell_type": "code", "execution_count": null, "id": "31c4a7d2", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.1-runcell04\n", "\n", "def log_like_sig(params, y, x):\n", "    \"\"\" Log-likehood function for a Gaussian bump (amp_s, mu_s) on top of a fixed PL background.\n", "    \"\"\"\n", "    amp_s, mu_s = params\n", "    std_s, amp_b, exp_b = 0.05, 50, -0.5\n", "    mu = bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b)\n", "    return poisson.logpmf(x, mu).sum()\n", "\n", "log_like_sig([50, 0.8], y, x)"]}, {"cell_type": "code", "execution_count": null, "id": "575b15f6", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.1-runcell05\n", "\n", "# Initial guess for the parameters\n", "initial_guess = [100., 0.1]\n", "\n", "# Set up the minimizer_kwargs for the basinhopping algorithm\n", "minimizer_kwargs = {\"method\": \"L-BFGS-B\", \"bounds\": ((0, 200), (0, 1))}\n", "\n", "# Perform the optimization using basinhopping\n", "opt = basinhopping(lambda thetas: -log_like_sig(thetas, y, x), initial_guess, minimizer_kwargs=minimizer_kwargs)\n", "\n", "print(\"MLE parameters: {}; true parameters: {}\".format(opt.x, (50, 0.8)))\n"]}, {"cell_type": "code", "execution_count": null, "id": "c369f9ef", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.1-runcell06\n", "\n", "def log_prior(thetas):\n", "    \"\"\" Log-prior function for a Gaussian bump (amp_s, mu_s) on top of a fixed PL background.\n", "    \"\"\"\n", "    amp_s, mu_s = thetas\n", "    if 0 < amp_s < 200 and 0 < mu_s < 2:\n", "        return 0\n", "    else:\n", "        return -np.inf\n", "    \n", "def log_post(thetas, y, x):\n", "    \"\"\" Log-posterior function for a Gaussian bump (amp_s, mu_s) on top of a fixed PL background.\n", "    \"\"\"\n", "    lp = log_prior(thetas)\n", "    if not np.isfinite(lp):\n", "        return -np.inf\n", "    else:\n", "        return lp + log_like_sig(thetas, y, x)\n", "    \n", "# Sampling with `emcee`\n", "ndim, nwalkers = 2, 32\n", "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_post, args=(y, x))\n", "\n", "pos = opt.x + 1e-3 * np.random.randn(nwalkers, ndim)\n", "sampler.run_mcmc(pos, 5000, progress=True);"]}, {"cell_type": "code", "execution_count": null, "id": "30032fe3", "metadata": {"tags": ["lect_01", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.1-runcell07\n", "\n", "# Plot posterior samples\n", "flat_samples = sampler.get_chain(discard=1000, flat=True)\n", "\n", "#make the corner plot with true values as lines\n", "corner.corner(flat_samples, labels=[\"amp_s\", \"mu_s\"], truths=[50, 0.8], smooth=.1);\n"]}, {"cell_type": "markdown", "id": "d6ad3f5d", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_24_1'></a>     \n", "\n", "| [Top](#section_24_0) | [Restart Section](#section_24_1) | [Next Section](#section_24_2) |\n"]}, {"cell_type": "markdown", "id": "24b1aca7", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 24.1.1</span>\n", "\n", "The calculations in this section only inferred two of the signal parameters, $A_s$ (`amp_s`) and $\\mu_s$ (`mu_s`). Complete the code below to also infer $\\sigma_s$ (`std_s`) via likelihood maximization (negative log-likelihood minimization). Report your result for the value of $\\sigma_s$ as a number with precision `1e-4`.\n", "\n", "<br>"]}, {"cell_type": "code", "execution_count": null, "id": "df56c003", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L24.1.1\n", "\n", "def log_like_sig2(params, y, x):\n", "    #Log-likehood function for a Gaussian bump (amp_s, mu_s, std_s) on top of a fixed PL background.\n", "    \n", "    #YOUR CODE HERE: make std_s one of the params\n", "    return poisson.logpmf(x, mu).sum()\n", "\n", "\n", "# Initial guess for the parameters\n", "initial_guess = [100., 0.1, 0.01]\n", "\n", "# Set up the minimizer_kwargs for the basinhopping algorithm\n", "minimizer_kwargs = {\"method\": \"L-BFGS-B\", \"bounds\": ((0, 200), (0, 1), (0.001, .2))}\n", "\n", "# Perform the optimization using basinhopping\n", "opt2 = basinhopping(lambda thetas: -log_like_sig2(thetas, y, x), initial_guess, minimizer_kwargs=minimizer_kwargs)\n", "\n", "print(\"MLE parameters: {}; true parameters: {}\".format(opt2.x, (50, 0.8, 0.05)))"]}, {"cell_type": "markdown", "id": "787806e2", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 24.1.2</span>\n", "\n", "Now generate the posterior distributions by running MCMC on the 3 parameters, as we did above for the two parameters. Make a corner plot, as done at the end of this section. Which parameters appear to be correlated?\n", "\n", "A) `amp_s` and `mu_s`\n", "\n", "B) `amp_s` and `std_s`\n", "\n", "C) `mu_s` and `std_s`\n", "\n", "<br>"]}, {"cell_type": "code", "execution_count": null, "id": "f03bb956", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L24.1.2\n", "\n", "def log_prior2(thetas):\n", "    #Log-prior function for a Gaussian bump (amp_s, mu_s, std_s) on top of a fixed PL background.\n", "    \n", "    #YOUR CODE HERE\n", "\n", "def log_post2(thetas, y, x):\n", "    #Log-posterior function for a Gaussian bump (amp_s, mu_s, std_s) on top of a fixed PL background.\n", "    \n", "    #YOUR CODE HERE\n", "    \n", "    \n", "# Sampling with `emcee`\n", "ndim, nwalkers = 3, 32\n", "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_post2, args=(y, x))\n", "\n", "# using basinhopping optimization from EXERCISE: L24.1.1 (i.e., opt2)\n", "pos = opt2.x + 1e-3 * np.random.randn(nwalkers, ndim)\n", "sampler.run_mcmc(pos, 5000, progress=True);\n", "\n", "# Plot posterior samples\n", "#YOUR CODE HERE"]}, {"cell_type": "markdown", "id": "e2786b06", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_24_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L24.2 Implicit Likelihood Method 1: Approximate Bayesian Computation</h2>  \n", "\n", "| [Top](#section_24_0) | [Previous Section](#section_24_1) | [Exercises](#exercises_24_2) | [Next Section](#section_24_3) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "aa56d949", "metadata": {"tags": ["lect_02", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.2-runcell01\n", "\n", "def bump_simulator(thetas, y):\n", "    \"\"\" Simulate samples from the bump forward model given theta = (amp_s, mu_s) and abscissa points y.\n", "    \"\"\"\n", "    amp_s, mu_s = thetas\n", "    std_s, amp_b, exp_b = 0.05, 50, -0.5\n", "    x_mu = bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b)\n", "    x = np.random.poisson(x_mu)\n", "    return x\n", "\n", "#we had defined this funciton previously\n", "def bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b):\n", "    \"\"\" Forward model for a Gaussian bump (amp_s, mu_s, std_s) on top of a power-law background (amp_b, exp_b).\n", "    \"\"\"\n", "    x_b = amp_b * (y ** exp_b)  # Power-law background\n", "    x_s = amp_s * np.exp(-((y - mu_s) ** 2) / (2 * std_s ** 2))  # Gaussian signal\n", "\n", "    x = x_b + x_s  # Total mean signal\n", "\n", "    return x\n", "\n", "# Test it out\n", "y = np.linspace(0.1, 1, 50)\n", "bump_simulator([50, 0.8], y)"]}, {"cell_type": "code", "execution_count": null, "id": "49536412", "metadata": {"tags": ["lect_02", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.2-runcell02\n", "\n", "#define the simulated data again, if you have not done so in the previous section\n", "\n", "def poisson_interval(k, alpha=0.32): \n", "    \"\"\" Uses chi2 to get the poisson interval.\n", "    \"\"\"\n", "    a = alpha\n", "    low, high = (chi2.ppf(a/2, 2*k) / 2, chi2.ppf(1-a/2, 2*k + 2) / 2)\n", "    if k == 0: \n", "        low = 0.0\n", "    return k - low, high - k\n", "\n", "# Mean expected counts\n", "x_mu = bump_forward_model(y, \n", "                    amp_s=50, mu_s=0.8, std_s=0.05,  # Signal params\n", "                    amp_b=50, exp_b=-0.5)  # Background params\n", "\n", "# Realized counts\n", "np.random.seed(42)\n", "x = np.random.poisson(x_mu)\n", "x_err = np.array([poisson_interval(k) for k in x.T]).T"]}, {"cell_type": "code", "execution_count": null, "id": "f22c64e1", "metadata": {"tags": ["lect_02", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.2-runcell03\n", "\n", "x_fwd = bump_simulator([50, 0.8], y)\n", "eps = np.sum(np.abs(x - x_fwd) ** 2) / len(x)\n", "eps"]}, {"cell_type": "code", "execution_count": null, "id": "dea9c014", "metadata": {"tags": ["lect_02", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.2-runcell04\n", "\n", "def abc(y, x, eps_thresh=500., n_samples=1000):\n", "    \"\"\"ABC algorithm for Gaussian bump model.\n", "\n", "    Args:\n", "        y (np.ndarray): Abscissa points.\n", "        x (np.ndarray): Data counts.\n", "        eps_thresh (float, optional): Acceptance threshold. Defaults to 500.0.\n", "        n_samples (int, optional): Number of samples after which to stop. Defaults to 1000.\n", "\n", "    Returns:\n", "        np.ndarray: Accepted samples approximating the posterior p(theta|x).\n", "    \"\"\"\n", "    samples = []\n", "    total_attempts = 0\n", "    progress_bar = tqdm(total=n_samples, desc=\"Accepted Samples\", unit=\"samples\")\n", "\n", "    # Keep simulating until we have enough accepted samples\n", "    while len(samples) < n_samples:\n", "        params = np.random.uniform(low=[0, 0], high=[200, 1])  # Priors; theta ~ p(theta)\n", "        x_fwd = bump_simulator(params, y)  # x ~ p(x|theta)\n", "        eps = np.sum(np.abs(x - x_fwd) ** 2) / len(x)  # Distance metric; d(x, x_fwd)\n", "        total_attempts += 1\n", "\n", "        # If accepted, add to samples\n", "        if eps < eps_thresh:\n", "            samples.append(params)\n", "            progress_bar.update(1)\n", "            acceptance_ratio = len(samples) / total_attempts\n", "            progress_bar.set_postfix(acceptance_ratio=f\"{acceptance_ratio:.3f}\")\n", "\n", "    progress_bar.close()\n", "    return np.array(samples)\n", "\n", "n_samples = 5_000\n", "post_samples = abc(y, x, eps_thresh=200, n_samples=n_samples)"]}, {"cell_type": "code", "execution_count": null, "id": "4905bd0b", "metadata": {"tags": ["lect_02", "learner", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L24.2-runcell05\n", "\n", "#note: here we also plot `flat_samples`, which was defined in cell `L24.1-runcell07`\n", "fig = corner.corner(post_samples, labels=[\"amp_s\", \"mu_s\"], truths=[50, 0.8], range=[(0, 200), (0.3, 1)], bins=50);\n", "corner.corner(flat_samples, labels=[\"amp_s\", \"mu_s\"], truths=[50, 0.8], fig=fig, color=\"red\", weights=np.ones(len(flat_samples)) * n_samples / len(flat_samples), range=[(0, 200), (0.3, 1)], bins=50);"]}, {"cell_type": "markdown", "id": "1fc20d8c", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='exercises_24_2'></a>     \n", "\n", "| [Top](#section_24_0) | [Restart Section](#section_24_2) | [Next Section](#section_24_3) |\n"]}, {"cell_type": "markdown", "id": "8db642b6", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 24.2.1</span>\n", "\n", "We can attempt to use our knowledge of MSE to yield a more biased approach to finding the best fit parameters. What we can do is weight the parameers by `1./eps`. Thus, larger values of `eps`, that are closer to acceptance threshold, will be weighted LESS. Complete the code below in order to make corner plots. What happens to the best fit parameters?\n", "\n", "A) The algorithm finds the same best fit parameters.\\\n", "B) The algorithm cannot find the best fit parameters.\\\n", "C) The algorithm finds a narrower range for `amp_s` than `mu_s`.\\\n", "D) The algorithm finds a narrower range for `mu_s` than `amp_s`.\n", "\n", "<br>"]}, {"cell_type": "code", "execution_count": null, "id": "7277ff9a", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L24.2.1\n", "\n", "def abc_weight(y, x, n_samples=1000,eps_thresh=500):\n", "    #ABC algorithm for Gaussian bump model.\n", "    #\n", "    #Args:\n", "    #    y (np.ndarray): Abscissa points.\n", "    #    x (np.ndarray): Data counts.\n", "    #    eps_thresh (float, optional): Acceptance threshold. Defaults to 500.0.\n", "    #    n_samples (int, optional): Number of samples after which to stop. Defaults to 1000.\n", "    #\n", "    #Returns:\n", "    #    np.ndarray: Accepted samples approximating the posterior p(theta|x).\n", "\n", "    samples = []\n", "    weights = []\n", "    total_attempts = 0\n", "    progress_bar = tqdm(total=n_samples, desc=\"Accepted Samples\", unit=\"samples\")\n", "\n", "    # Keep simulating until we have enough accepted samples\n", "    while len(samples) < n_samples:\n", "        params = np.random.uniform(low=[0, 0], high=[200, 1])  # Priors; theta ~ p(theta)\n", "        x_fwd = bump_simulator(params, y)  # x ~ p(x|theta)\n", "        eps = np.sum(np.abs(x - x_fwd) ** 2) / len(x)  # Distance metric; d(x, x_fwd)\n", "        total_attempts += 1\n", "\n", "        # If accepted, add to samples\n", "        if eps < eps_thresh:\n", "            samples.append(params)\n", "            weights.append(#YOUR CODE HERE) #choose weights proportional to 1/eps\n", "            progress_bar.update(1)\n", "            acceptance_ratio = len(samples) / total_attempts\n", "            progress_bar.set_postfix(acceptance_ratio=f\"{acceptance_ratio:.3f}\")\n", "\n", "    progress_bar.close()\n", "    return np.array(samples), np.array(weights)*len(samples)/np.sum(weights)\n", "\n", "n_samples = 5_000\n", "post_samples_weight,sample_weights = abc_weight(y, x, n_samples=n_samples)\n", "\n", "fig = corner.corner(post_samples_weight,weights=sample_weights, labels=[\"amp_s\", \"mu_s\"], truths=[50, 0.8], range=[(0, 200), (0.3, 1)], bins=50);\n", "corner.corner(flat_samples, labels=[\"amp_s\", \"mu_s\"], truths=[50, 0.8], fig=fig, color=\"red\", weights=np.ones(len(flat_samples)) * n_samples / len(flat_samples), range=[(0, 200), (0.3, 1)], bins=50);\n"]}, {"cell_type": "markdown", "id": "a08284ad", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_24_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L24.3 Implicit Likelihood Method 2: Neural Likelihood-ratio Estimation</h2>  \n", "\n", "| [Top](#section_24_0) | [Previous Section](#section_24_2) | [Exercises](#exercises_24_3) | [Next Section](#section_24_4) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "f2ec8a3f", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell01\n", "\n", "n_train = 50_000\n", "\n", "# Simulate training data\n", "theta_samples = np.random.uniform(low=[0, 0], high=[200, 1], size=(n_train, 2))  # Parameter proposal\n", "x_samples = np.array([bump_simulator(theta, y) for theta in tqdm(theta_samples)])\n", "print(\"here is one set of random data poitns:\",x_samples[0])\n", "\n", "# Convert to torch tensors\n", "theta_samples = torch.tensor(theta_samples, dtype=torch.float32)\n", "x_samples = torch.tensor(x_samples, dtype=torch.float32)\n", "\n", "# Normalize the data to make the NN understand these guys\n", "x_mean = x_samples.mean(dim=0)\n", "x_std = x_samples.std(dim=0)\n", "x_samples = (x_samples - x_mean) / x_std\n", "\n", "theta_mean = theta_samples.mean(dim=0)\n", "theta_std = theta_samples.std(dim=0)\n", "theta_samples = (theta_samples - theta_mean) / theta_std"]}, {"cell_type": "code", "execution_count": null, "id": "6eb0e0e4", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell02\n", "\n", "def build_mlp(input_dim, hidden_dim, output_dim, layers, activation=nn.GELU()):\n", "    \"\"\"Create an MLP from the configuration.\"\"\"\n", "    seq = [nn.Linear(input_dim, hidden_dim), activation]\n", "    for _ in range(layers):\n", "        seq += [nn.Linear(hidden_dim, hidden_dim), activation]\n", "    seq += [nn.Linear(hidden_dim, output_dim)]\n", "    return nn.Sequential(*seq)"]}, {"cell_type": "code", "execution_count": null, "id": "83acd24a", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell03\n", "\n", "class NeuralRatioEstimator(pl.LightningModule):\n", "    \"\"\" Simple neural likelihood-to-evidence ratio estimator, using an MLP as a parameterized classifier.\n", "    \"\"\"\n", "    def __init__(self, x_dim, theta_dim):\n", "        super().__init__()\n", "        self.classifier = build_mlp(input_dim=x_dim + theta_dim, hidden_dim=128, output_dim=1, layers=4)\n", "\n", "    def forward(self, x):\n", "        return self.classifier(x)\n", "\n", "    def loss(self, x, theta):\n", "        #x is our input histogram, the first thing we do is clone it using repeat_interleave\n", "\n", "        # Repeat x in groups of 2 along batch axis\n", "        x = x.repeat_interleave(2, dim=0)\n", "\n", "        #Now we need to make a random permutation of the theta parameters to make an incorrect pairing\n", "        # Get a shuffled version of theta\n", "        theta_shuffled = theta[torch.randperm(theta.shape[0])]\n", "\n", "        # We insert this into our new theta array to get the correct an incorrect pairings\n", "        # Interleave theta and shuffled theta\n", "        theta = torch.stack([theta, theta_shuffled], dim=1).reshape(-1, theta.shape[1])\n", "\n", "        #Now we can define what is correct, and what is not labels[1::2]=0.0, will take [1,1,1,...]=>[1,0,1,0,..]\n", "        #(ie first is correct and 2nd is incorrect and so on)\n", "        # Get labels; ones for pairs from joint, zeros for pairs from marginals\n", "        labels = torch.ones(x.shape[0], device=x.device)\n", "        labels[1::2] = 0.0\n", "\n", "        # Pass through parameterized classifier to get logits\n", "        # now merge the inputs to x and theta into one vecotr and apply the mlp (note self function does this)\n", "        logits = self(torch.cat([x, theta], dim=1))\n", "        probs = torch.sigmoid(logits).squeeze()\n", "\n", "        #Finally binary cross entropy with our truth\n", "        return nn.BCELoss(reduction='none')(probs, labels)\n", "\n", "\n", "    def training_step(self, batch, batch_idx):\n", "        x, theta = batch\n", "        loss = self.loss(x, theta).mean()\n", "        self.log(\"train_loss\", loss)\n", "        return loss\n", "\n", "    def validation_step(self, batch, batch_idx):\n", "        x, theta = batch\n", "        loss = self.loss(x, theta).mean()\n", "        self.log(\"val_loss\", loss)\n", "        return loss\n", "\n", "    def configure_optimizers(self):\n", "        return torch.optim.Adam(self.parameters(), lr=3e-4)"]}, {"cell_type": "code", "execution_count": null, "id": "806b7557", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell04\n", "\n", "# Lets test out the above code, by taking 64 random histograms (x_samples[:64]) and computing the loss of the MLP\n", "# Evaluate loss; initially it should be around -log(0.5) = 0.693\n", "nre = NeuralRatioEstimator(x_dim=50, theta_dim=2)\n", "nre.loss(x_samples[:64], theta_samples[:64])"]}, {"cell_type": "code", "execution_count": null, "id": "cfb97306", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell05\n", "\n", "val_fraction = 0.1\n", "batch_size = 128\n", "n_samples_val = int(val_fraction * len(x_samples))\n", "\n", "dataset = TensorDataset(x_samples, theta_samples)\n", "\n", "dataset_train, dataset_val = random_split(dataset, [len(x_samples) - n_samples_val, n_samples_val])\n", "train_loader = DataLoader(dataset_train, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=True)\n", "val_loader = DataLoader(dataset_val, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=False)\n"]}, {"cell_type": "code", "execution_count": null, "id": "f63ff4ec", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell06\n", "\n", "trainer = pl.Trainer(max_epochs=12)\n", "trainer.fit(model=nre, train_dataloaders=train_loader, val_dataloaders=val_loader);"]}, {"cell_type": "code", "execution_count": null, "id": "46420b3b", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell07\n", "\n", "def log_like(theta, x):\n", "    \"\"\" Log-likelihood ratio estimator using trained classifier logits.\n", "    \"\"\"\n", "    #convert to torch and normalize inputs\n", "    x = torch.Tensor(x)\n", "    theta = torch.Tensor(theta)\n", "\n", "    # Normalize\n", "    x = (x - x_mean) / x_std\n", "    theta = (theta - theta_mean) / theta_std\n", "    #Ensures array input is correct\n", "    x = torch.atleast_1d(x)\n", "    theta = torch.atleast_1d(theta)\n", "    #now apply the NN\n", "    return nre.classifier(torch.cat([x, theta], dim=-1)).squeeze().detach().numpy()\n", "\n", "theta_test = np.array([90, 0.8])\n", "x_test = bump_simulator(theta_test, y)\n", "\n", "log_like(theta_test, x_test)"]}, {"cell_type": "code", "execution_count": null, "id": "377b0ce5", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell08\n", "\n", "def log_post(theta, x):\n", "    \"\"\" Log-posterior distribution, for sampling.\n", "    \"\"\"\n", "    lp = log_prior(theta)\n", "    if not np.isfinite(lp):\n", "        return -np.inf\n", "    else:\n", "        return lp + log_like(theta, x)"]}, {"cell_type": "code", "execution_count": null, "id": "180d9bfc", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell09\n", "\n", "ndim, nwalkers = 2, 32\n", "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_post, args=(x_test,))\n", "\n", "pos = opt.x + 1e-3 * np.random.randn(nwalkers, ndim)\n", "sampler.run_mcmc(pos, 2000, progress=True);"]}, {"cell_type": "code", "execution_count": null, "id": "fbb30eb9", "metadata": {"tags": ["learner", "learner_chopped", "lect_03"]}, "outputs": [], "source": ["#>>>RUN: L24.3-runcell10\n", "\n", "flat_samples_nre = sampler.get_chain(discard=1000, flat=True)\n", "corner.corner(flat_samples_nre, labels=[\"amp_s\", \"mu_s\"], truths=[90, 0.8], range=[(25, 150), (0.5, 1.)]);\n"]}, {"cell_type": "markdown", "id": "c43bc7db", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='exercises_24_3'></a>     \n", "\n", "| [Top](#section_24_0) | [Restart Section](#section_24_3) | [Next Section](#section_24_4) |\n"]}, {"cell_type": "markdown", "id": "5aff9d51", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 24.3.1</span>\n", "\n", "Complete the code below to compare the NRE approach with the previous explicit likelihood (EL) approach from section 1. How do the corner plots compare? Is one better?\n", "\n", "A) The results are comparable with no clear winner.\n", "\n", "B) The results are comparable, but the NRE method is better (producing a narrower range of results).\n", "\n", "C) The results are comparable, but the EL method is better (producing a narrower range of results).\n", "\n", "\n", "<br>"]}, {"cell_type": "code", "execution_count": null, "id": "b7447a3b", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L24.3.1\n", "\n", "#First create a figure with the result from `flat_samples_nre` above\n", "fig=corner.corner(flat_samples_nre, labels=[\"amp_s\", \"mu_s\"], truths=[90, 0.8], range=[(60, 120), (0.75, 0.85)]);\n", "\n", "def log_like_sig_old(params, y, x):\n", "    #Log-likehood function for a Gaussian bump (amp_s, mu_s) on top of a fixed PL background.\n", "\n", "    amp_s, mu_s = params\n", "    std_s, amp_b, exp_b = 0.05, 50, -0.5\n", "    mu = bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b)\n", "    return poisson.logpmf(x, mu).sum()\n", "\n", "def log_post_old(thetas, y, x):\n", "    #Log-posterior function for a Gaussian bump (amp_s, mu_s) on top of a fixed PL background.\n", "    \n", "    lp = log_prior(thetas)\n", "    if not np.isfinite(lp):\n", "        return -np.inf\n", "    else:\n", "        return lp + log_like_sig_old(thetas, y, x)\n", "\n", "    \n", "# Sampling with `emcee`\n", "## see  above, be sure to use x_test for the new x values and  log_post_old\n", "#YOUR CODE HERE\n", "\n", "\n", "#plot the new values from mcmc in red\n", "flat_samples_mcmc = sampler.get_chain(discard=1000, flat=True)\n", "corner.corner(flat_samples_mcmc,color='red', labels=[\"amp_s\", \"mu_s\"], truths=[90, 0.8],fig=fig, range=[(60, 120), (0.75, 0.85)]);\n"]}, {"cell_type": "markdown", "id": "b86f43e3", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 24.3.2</span>\n", "\n", "Now lets look into a situation where this approach starts to break down. Edit the variable `theta_samples_tmp` to set the amplitude range of our sample to be from 0 to 2000 (previously it was 0 to 200, defined earlier in this section). How does the fit look now?\n", "\n", "A) The fit is comparable or better.\n", "\n", "B) The fit is okay in `amp_s` but not `mu_s`.\n", "\n", "C) The fit is okay in `mu_s` but not `amp_s`.\n", "\n", "D) The fit is way off in both `amp_s` and `mu_s`.\n", "\n", "\n", "<br>"]}, {"cell_type": "code", "execution_count": null, "id": "b6c1d025", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L24.3.2\n", "\n", "n_train = 50_000\n", "\n", "# Simulate training data\n", "theta_samples_tmp = np.random.uniform(####YOUR CODE HERE####)  # Parameter proposal\n", "x_samples_tmp = np.array([bump_simulator(theta, y) for theta in tqdm(theta_samples_tmp)])\n", "\n", "# Convert to torch tensors\n", "theta_samples_tmp = torch.tensor(theta_samples_tmp, dtype=torch.float32)\n", "x_samples_tmp = torch.tensor(x_samples_tmp, dtype=torch.float32)\n", "\n", "# Normalize the data\n", "x_mean_tmp = x_samples_tmp.mean(dim=0)\n", "x_std_tmp = x_samples_tmp.std(dim=0)\n", "x_samples_tmp = (x_samples_tmp - x_mean_tmp) / x_std_tmp\n", "theta_mean_tmp = theta_samples_tmp.mean(dim=0)\n", "theta_std_tmp = theta_samples_tmp.std(dim=0)\n", "theta_samples_tmp = (theta_samples_tmp - theta_mean_tmp) / theta_std_tmp\n", "\n", "#training code\n", "val_fraction = 0.1\n", "batch_size = 128\n", "n_samples_val_tmp = int(val_fraction * len(x_samples_tmp))\n", "dataset_tmp = TensorDataset(x_samples_tmp, theta_samples_tmp)\n", "dataset_train_tmp, dataset_val_tmp = random_split(dataset_tmp, [len(x_samples_tmp) - n_samples_val_tmp, n_samples_val_tmp])\n", "train_loader_tmp = DataLoader(dataset_train_tmp, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=True)\n", "val_loader_tmp = DataLoader(dataset_val_tmp, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=False)#\n", "\n", "nre_largesample = NeuralRatioEstimator(x_dim=50, theta_dim=2)\n", "trainer = pl.Trainer(max_epochs=12)\n", "trainer.fit(model=nre_largesample, train_dataloaders=train_loader_tmp, val_dataloaders=val_loader_tmp);\n", "\n", "def log_like(theta, x):\n", "    \"\"\" Log-likelihood ratio estimator using trained classifier logits.\n", "    \"\"\"\n", "\n", "    x = torch.Tensor(x)\n", "    theta = torch.Tensor(theta)\n", "\n", "    # Normalize\n", "    x = (x - x_mean) / x_std\n", "    theta = (theta - theta_mean) / theta_std\n", "\n", "    x = torch.atleast_1d(x)\n", "    theta = torch.atleast_1d(theta)\n", "\n", "    return nre_largesample.classifier(torch.cat([x, theta], dim=-1)).squeeze().detach().numpy()\n", "\n", "def log_post(theta, x):\n", "    \"\"\" Log-posterior distribution, for sampling.\n", "    \"\"\"\n", "    lp = log_prior(theta)\n", "    if not np.isfinite(lp):\n", "        return -np.inf\n", "    else:\n", "        return lp + log_like(theta, x)\n", "\n", "ndim, nwalkers = 2, 32\n", "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_post, args=(x_test,))\n", "\n", "pos = opt.x + 1e-3 * np.random.randn(nwalkers, ndim)\n", "sampler.run_mcmc(pos, 2000, progress=True);\n", "\n", "flat_samples_nre2 = sampler.get_chain(discard=1000, flat=True)\n", "fig=corner.corner(flat_samples_nre2, labels=[\"amp_s\", \"mu_s\"], truths=[90, 0.8], range=[(60, 120), (0.75, 0.85)]);\n"]}, {"cell_type": "markdown", "id": "72c70b9f", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_24_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L24.4 Implicit Likelihood Method 3: Neural Posterior Estimation</h2>  \n", "\n", "| [Top](#section_24_0) | [Previous Section](#section_24_3) | [Exercises](#exercises_24_4) | [Next Section](#section_24_5) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "543f5af9", "metadata": {"tags": ["learner", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L24.4-runcell01\n", "\n", "from nflows.flows.base import Flow\n", "from nflows.distributions.normal import StandardNormal\n", "from nflows.transforms.base import CompositeTransform\n", "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform\n", "from nflows.transforms.permutations import ReversePermutation"]}, {"cell_type": "code", "execution_count": null, "id": "150d56ee", "metadata": {"tags": ["learner", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L24.4-runcell02\n", "\n", "def get_flow(d_in=2, d_hidden=32, d_context=16, n_layers=4):\n", "    \"\"\" Instantiate a simple (Masked Autoregressive) normalizing flow.\n", "    \"\"\"\n", "\n", "    base_dist = StandardNormal(shape=[d_in])\n", "\n", "    transforms = []\n", "    for _ in range(n_layers):\n", "        transforms.append(ReversePermutation(features=d_in))\n", "        transforms.append(MaskedAffineAutoregressiveTransform(features=d_in, hidden_features=d_hidden, context_features=d_context))\n", "    transform = CompositeTransform(transforms)\n", "\n", "    flow = Flow(transform, base_dist)\n", "    return flow\n", "\n", "# Instantiate flow\n", "flow = get_flow()\n", "\n", "# Make sure sampling and log-prob calculation makes sense\n", "samples, log_prob = flow.sample_and_log_prob(num_samples=100, context=torch.randn(2, 16))\n", "print(samples.shape, log_prob.shape)"]}, {"cell_type": "code", "execution_count": null, "id": "e7ecdb56", "metadata": {"tags": ["learner", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L24.4-runcell03\n", "\n", "class NeuralPosteriorEstimator(pl.LightningModule):\n", "    \"\"\" Simple neural posterior estimator class using a normalizing flow as the posterior density estimator.\n", "    \"\"\"\n", "    def __init__(self, featurizer, d_context=16,d_hidden=32):\n", "        super().__init__()\n", "        self.featurizer = featurizer\n", "        self.flow = get_flow(d_in=2, d_hidden=d_hidden, d_context=d_context, n_layers=4)\n", "\n", "    def forward(self, x):\n", "        return self.featurizer(x)\n", "    \n", "    def loss(self, x, theta):\n", "        context = self(x)\n", "        return -self.flow.log_prob(inputs=theta, context=context)\n", "\n", "    def training_step(self, batch, batch_idx):\n", "        x, theta = batch\n", "        loss = self.loss(x, theta).mean()\n", "        self.log(\"train_loss\", loss)\n", "        return loss\n", "\n", "    def validation_step(self, batch, batch_idx):\n", "        x, theta = batch\n", "        loss = self.loss(x, theta).mean()\n", "        self.log(\"val_loss\", loss)\n", "        return loss\n", "\n", "    def configure_optimizers(self):\n", "        return torch.optim.Adam(self.parameters(), lr=3e-4)"]}, {"cell_type": "code", "execution_count": null, "id": "9ffb4176", "metadata": {"tags": ["learner", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L24.4-runcell04\n", "\n", "npe = NeuralPosteriorEstimator(featurizer=build_mlp(input_dim=50, hidden_dim=128, output_dim=16, layers=4))\n", "npe.loss(x_samples[:64], theta_samples[:64])"]}, {"cell_type": "code", "execution_count": null, "id": "979538a5", "metadata": {"tags": ["learner", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L24.4-runcell05\n", "\n", "trainer = pl.Trainer(max_epochs=12)\n", "trainer.fit(model=npe, train_dataloaders=train_loader, val_dataloaders=val_loader);"]}, {"cell_type": "code", "execution_count": null, "id": "42923c77", "metadata": {"tags": ["learner", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L24.4-runcell06\n", "\n", "theta_test = np.array([90, 0.8])\n", "x_test = bump_simulator(theta_test, y)"]}, {"cell_type": "code", "execution_count": null, "id": "8c97ea6e", "metadata": {"tags": ["learner", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L24.4-runcell07\n", "\n", "x_test_norm = (torch.Tensor(x_test) - x_mean) / x_std\n", "context = npe.featurizer(x_test_norm).unsqueeze(0)"]}, {"cell_type": "code", "execution_count": null, "id": "34934952", "metadata": {"tags": ["learner", "learner_chopped", "lect_04"]}, "outputs": [], "source": ["#>>>RUN: L24.4-runcell08\n", "\n", "samples_test_npe = npe.flow.sample(num_samples=2000, context=context) * theta_std + theta_mean\n", "samples_test_npe = samples_test_npe.detach().numpy()\n", "corner.corner(samples_test_npe, labels=[\"amp_s\", \"mu_s\"], truths=[90, 0.8]);"]}, {"cell_type": "markdown", "id": "d3747380", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='exercises_24_4'></a>     \n", "\n", "| [Top](#section_24_0) | [Restart Section](#section_24_4) | [Next Section](#section_24_5) |\n"]}, {"cell_type": "markdown", "id": "b4e479ae", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 24.4.1</span>\n", "\n", "In both neural ratio and neural posterior estimation, what role does the feature extractor (which was an MLP in the code above) play? Select ALL that apply.\n", "\n", "A) Extract features that are informative of the parameters of interest\\\n", "B) Increase the complexity of the model to improve performance\\\n", "C) Normalize the data to a common scale\\\n", "D) Remove noise from the data to improve estimation accuracy\\\n", "E) Reduce the dimensionality of the data\n", "\n", "\n", "<br>"]}, {"cell_type": "markdown", "id": "27e151bf", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 24.4.2</span>\n", "\n", "Let's check the limitations of this tool as an estimator. Run the code below, where we shrink the number of parameters in the MLP used for the normalizing flow by an order of magnitude, then plot the results in red alongside the results from `L24.4-runcell08` above. How does this perform?\n", "\n", "A) This performs better than the previous NPE.\n", "\n", "B) This performs just as well as the previous NPE.\n", "\n", "C) This performs more poorly than the previous NPE.\n", "\n", "\n", "<br>"]}, {"cell_type": "code", "execution_count": null, "id": "ea8688a7", "metadata": {"tags": ["py", "draft", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L24.4.2\n", "\n", "#first create a corner plot with the results from `L24.4-runcell08`\n", "fig = corner.corner(samples_test_npe, labels=[\"amp_s\", \"mu_s\"], truths=[90, 0.8]);\n", "\n", "#now define a new NPE with fewer parameters\n", "npe_shrink = NeuralPosteriorEstimator(d_context=2,d_hidden=4,featurizer=build_mlp(input_dim=50, hidden_dim=2, output_dim=2, layers=1))\n", "\n", "\n", "npe_shrink.loss(x_samples[:64], theta_samples[:64])\n", "trainer = pl.Trainer(max_epochs=12)\n", "trainer.fit(model=npe_shrink, train_dataloaders=train_loader, val_dataloaders=val_loader);\n", "\n", "context_shrink = npe_shrink.featurizer(x_test_norm).unsqueeze(0)\n", "samples_test_shrink = npe_shrink.flow.sample(num_samples=2000, context=context_shrink) * theta_std + theta_mean\n", "samples_test_shrink = samples_test_shrink.detach().numpy()[0]\n", "\n", "#plot the new results in red\n", "corner.corner(samples_test_shrink, color='red', fig=fig, labels=[\"amp_s\", \"mu_s\"], truths=[90, 0.8]);"]}, {"cell_type": "markdown", "id": "62bd03d6", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_24_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L24.5 A more complicated example: distribution of point sources in a 2D image</h2>  \n", "\n", "| [Top](#section_24_0) | [Previous Section](#section_24_4) | [Exercises](#exercises_24_5) | [Next Section](#section_24_6) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "b9a7c978", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell01\n", "\n", "from scipy.stats import binned_statistic_2d\n", "from astropy.convolution import convolve, Gaussian2DKernel\n", "\n", "\n", "def simulate_sources(amp_b, exp_b, s_min=0.5, s_max=50.0, box_size=1., resolution=64, sigma_psf=0.01):\n", "    \"\"\" Simulate a map of point sources with mean counts drawn from a power law (Pareto) distribution dn/ds = amp_b * s ** exp_b\n", "    \"\"\"\n", "    # Get number of sources by analytically integrating dn/ds and taking Poisson realization\n", "    n_sources = np.random.poisson(-amp_b * (s_min ** (exp_b - 1)) / (exp_b - 1))\n", "\n", "    # Draw fluxes from truncated power law amp_b * s ** (exp_b - 1), with s_min and s_max as the bounds\n", "    fluxes = draw_powerlaw_flux(n_sources, s_min, s_max, exp_b)\n", "\n", "    positions = np.random.uniform(0., box_size, size=(n_sources, 2))\n", "    bins = np.linspace(0, box_size, resolution + 1)\n", "\n", "    pixel_size = box_size / resolution\n", "    kernel = Gaussian2DKernel(x_stddev=1.0 * sigma_psf / pixel_size)\n", "\n", "    mu_signal = binned_statistic_2d(x=positions[:, 0], y=positions[:, 1], values=fluxes, statistic='sum', bins=bins).statistic\n", "    counts = np.random.poisson(convolve(mu_signal, kernel))\n", "                \n", "    return fluxes, counts\n", "\n", "def draw_powerlaw_flux(n_sources, s_min, s_max, exp_b):\n", "    \"\"\"\n", "    Draw from a power law with slope `exp_b` and min/max mean counts `s_min` and `s_max`. From:\n", "    https://stackoverflow.com/questions/31114330/python-generating-random-numbers-from-a-power-law-distribution\n", "    \"\"\"\n", "    u = np.random.uniform(0, 1, size=n_sources)\n", "    s_low_u, s_high_u = s_min ** (exp_b + 1), s_max ** (exp_b + 1)\n", "    return (s_low_u + (s_high_u - s_low_u) * u) ** (1.0 / (exp_b + 1.0))\n", "\n", "fluxes, counts = simulate_sources(amp_b=200., exp_b=-1.2)\n", "plt.imshow(counts, cmap='viridis', vmax=20)\n", "plt.xlabel(\"Pixels\")\n", "plt.ylabel(\"Pixels\")"]}, {"cell_type": "code", "execution_count": null, "id": "cfeb8118", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell02\n", "\n", "# Draw parameters from the prior\n", "n_params = 16\n", "\n", "amp_b_prior = (100., 300.)\n", "exp_b_prior = (-2.0, -0.5)\n", "\n", "amp_bs = np.random.uniform(amp_b_prior[0], amp_b_prior[1], n_params)\n", "exp_bs = np.random.uniform(exp_b_prior[0], exp_b_prior[1], n_params)\n", "\n", "# Plot the data samples on a grid\n", "fig, axes = plt.subplots(4, 4, figsize=(14, 14))\n", "\n", "for i, ax in enumerate(axes.flatten()):\n", "    fluxes, counts = simulate_sources(amp_b=amp_bs[i], exp_b=exp_bs[i])\n", "    im = ax.imshow(counts, cmap='viridis', vmax=20)\n", "    ax.set_title(f'$A_b={amp_bs[i]:.2f}, n_b={exp_bs[i]:.2f}$')\n", "    ax.set_xticks([])\n", "    ax.set_yticks([])\n", "\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "ee9d98d3", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell03\n", "\n", "n_train = 20_000\n", "\n", "# Sample from prior, then simulate\n", "theta_samples = np.random.uniform(low=[10., -3.], high=[200., -0.99], size=(n_train, 2))\n", "x_samples = np.array([simulate_sources(theta[0], theta[1])[1] for theta in tqdm(theta_samples)])\n", "\n", "# Convert to torch tensors\n", "theta_samples = torch.Tensor(theta_samples)\n", "x_samples = torch.Tensor(x_samples)\n", "\n", "# Normalize the data\n", "x_mean = x_samples.mean(dim=0)\n", "x_std = x_samples.std(dim=0)\n", "x_samples = (x_samples - x_mean) / x_std\n", "\n", "theta_mean = theta_samples.mean(dim=0)\n", "theta_std = theta_samples.std(dim=0)\n", "theta_samples = (theta_samples - theta_mean) / theta_std\n"]}, {"cell_type": "code", "execution_count": null, "id": "38814878", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell04\n", "\n", "val_fraction = 0.1\n", "batch_size = 64\n", "n_samples_val = int(val_fraction * len(x_samples))\n", "\n", "dataset = TensorDataset(x_samples, theta_samples)\n", "\n", "dataset_train, dataset_val = random_split(dataset, [len(x_samples) - n_samples_val, n_samples_val])\n", "train_loader = DataLoader(dataset_train, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=True)\n", "val_loader = DataLoader(dataset_val, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=False)\n"]}, {"cell_type": "code", "execution_count": null, "id": "e30772a3", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell05\n", "\n", "class CNN(nn.Module):\n", "    \"\"\" Simple CNN feature extractor.\n", "    \"\"\"\n", "    def __init__(self, output_dim):\n", "        super(CNN, self).__init__()\n", "\n", "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n", "        self.pool1 = nn.MaxPool2d(2, 2)\n", "\n", "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n", "        self.pool2 = nn.MaxPool2d(2, 2)\n", "\n", "        self.fc1 = nn.Linear(16 * 16 * 16, 64)\n", "        self.fc2 = nn.Linear(64, output_dim)\n", "\n", "    def forward(self, x):\n", "        \n", "        x = x.unsqueeze(1)  # Add channel dim\n", "        \n", "        x = self.pool1(F.leaky_relu(self.conv1(x), negative_slope=0.02))\n", "        x = self.pool2(F.leaky_relu(self.conv2(x), negative_slope=0.02))\n", "\n", "        x = x.view(x.size(0), -1)\n", "\n", "        x = F.leaky_relu(self.fc1(x), negative_slope=0.01)\n", "        x = self.fc2(x)\n", "\n", "        return x\n", "\n", "\n", "#functions defined in previous sections, redefined here\n", "from nflows.flows.base import Flow\n", "from nflows.distributions.normal import StandardNormal\n", "from nflows.transforms.base import CompositeTransform\n", "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform\n", "from nflows.transforms.permutations import ReversePermutation\n", "\n", "def get_flow(d_in=2, d_hidden=32, d_context=16, n_layers=4):\n", "    \"\"\" Instantiate a simple (Masked Autoregressive) normalizing flow.\n", "    \"\"\"\n", "\n", "    base_dist = StandardNormal(shape=[d_in])\n", "\n", "    transforms = []\n", "    for _ in range(n_layers):\n", "        transforms.append(ReversePermutation(features=d_in))\n", "        transforms.append(MaskedAffineAutoregressiveTransform(features=d_in, hidden_features=d_hidden, context_features=d_context))\n", "    transform = CompositeTransform(transforms)\n", "\n", "    flow = Flow(transform, base_dist)\n", "    return flow\n", "\n", "\n", "class NeuralPosteriorEstimator(pl.LightningModule):\n", "    \"\"\" Simple neural posterior estimator class using a normalizing flow as the posterior density estimator.\n", "    \"\"\"\n", "    def __init__(self, featurizer, d_context=16,d_hidden=32):\n", "        super().__init__()\n", "        self.featurizer = featurizer\n", "        self.flow = get_flow(d_in=2, d_hidden=d_hidden, d_context=d_context, n_layers=4)\n", "\n", "    def forward(self, x):\n", "        return self.featurizer(x)\n", "    \n", "    def loss(self, x, theta):\n", "        context = self(x)\n", "        return -self.flow.log_prob(inputs=theta, context=context)\n", "\n", "    def training_step(self, batch, batch_idx):\n", "        x, theta = batch\n", "        loss = self.loss(x, theta).mean()\n", "        self.log(\"train_loss\", loss)\n", "        return loss\n", "\n", "    def validation_step(self, batch, batch_idx):\n", "        x, theta = batch\n", "        loss = self.loss(x, theta).mean()\n", "        self.log(\"val_loss\", loss)\n", "        return loss\n", "\n", "    def configure_optimizers(self):\n", "        return torch.optim.Adam(self.parameters(), lr=3e-4)"]}, {"cell_type": "code", "execution_count": null, "id": "0f5154a6", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell06\n", "\n", "npe = NeuralPosteriorEstimator(featurizer=CNN(output_dim=32), d_context=32)\n", "npe.loss(x_samples[:64], theta_samples[:64])"]}, {"cell_type": "code", "execution_count": null, "id": "c6989879", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell07\n", "\n", "trainer = pl.Trainer(max_epochs=15)\n", "trainer.fit(model=npe, train_dataloaders=train_loader, val_dataloaders=val_loader);"]}, {"cell_type": "code", "execution_count": null, "id": "a70ba905", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell08\n", "\n", "npe = npe.eval()"]}, {"cell_type": "code", "execution_count": null, "id": "53369584", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell09\n", "\n", "params_test = np.array([15., -1.4])\n", "x_test = simulate_sources(params_test[0], params_test[1])[1]"]}, {"cell_type": "code", "execution_count": null, "id": "645150db", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell10\n", "\n", "x_test_norm = (torch.Tensor(x_test) - x_mean) / x_std\n", "context = npe.featurizer(x_test_norm.unsqueeze(0))\n", "\n", "samples_test = npe.flow.sample(num_samples=10000, context=context) * theta_std + theta_mean\n", "samples_test = samples_test.detach().numpy()\n", "\n", "corner.corner(samples_test, labels=[\"amp\", \"exp\"], truths=params_test);"]}, {"cell_type": "code", "execution_count": null, "id": "c4bb1017", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell11\n", "\n", "n_test = 200  # How many test samples to draw for coverage test\n", "\n", "# Get samples \n", "x_test = torch.Tensor([simulate_sources(params_test[0], params_test[1])[1] for _ in range(n_test)])\n", "x_test_norm = (torch.Tensor(x_test) - x_mean) / x_std\n", "\n", "# and featurize\n", "context = npe.featurizer(x_test_norm)\n", "\n", "# Get posterior for all samples together in a batch\n", "samples_test = npe.flow.sample(num_samples=1000, context=context) * theta_std + theta_mean\n", "samples_test = samples_test.detach().numpy()"]}, {"cell_type": "code", "execution_count": null, "id": "3b609755", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell12\n", "\n", "def hpd(samples, credible_mass=0.95):\n", "    \"\"\"Compute highest posterior density (HPD) of array for given credible mass.\"\"\"\n", "    sorted_samples = np.sort(samples)\n", "    interval_idx_inc = int(np.floor(credible_mass * sorted_samples.shape[0]))\n", "    n_intervals = sorted_samples.shape[0] - interval_idx_inc\n", "    interval_width = np.zeros(n_intervals)\n", "    for i in range(n_intervals):\n", "        interval_width[i] = sorted_samples[i + interval_idx_inc] - sorted_samples[i]\n", "    hdi_min = sorted_samples[np.argmin(interval_width)]\n", "    hdi_max = sorted_samples[np.argmin(interval_width) + interval_idx_inc]\n", "    return hdi_min, hdi_max\n", "\n", "hpd(samples_test[0, :, 0], credible_mass=0.2)"]}, {"cell_type": "code", "execution_count": null, "id": "d06307f0", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell13\n", "\n", "p_nominals = np.linspace(0.01, 0.99, 50)\n", "contains_true = np.zeros((2, n_test, len(p_nominals)))\n", "\n", "for i_param in range(2):\n", "    for i, sample in enumerate(samples_test[:, :, i_param]):\n", "        for j, p_nominal in enumerate(p_nominals):\n", "            hdi_min, hdi_max = hpd(sample, credible_mass=p_nominal)\n", "            if hdi_min < params_test[i_param] < hdi_max:\n", "                contains_true[i_param, i, j] = 1"]}, {"cell_type": "code", "execution_count": null, "id": "d8f9c24a", "metadata": {"tags": ["learner", "learner_chopped", "lect_05"]}, "outputs": [], "source": ["#>>>RUN: L24.5-runcell14\n", "\n", "# Make two plots, one for each parameter\n", "\n", "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n", "\n", "ax[0].plot(p_nominals, contains_true[0].sum(0) / n_test)\n", "ax[0].plot([0, 1], [0, 1], color=\"black\", linestyle=\"--\")\n", "ax[0].set_xlabel(\"Nominal coverage\")\n", "ax[0].set_ylabel(\"Empirical coverage\")\n", "ax[0].set_title(\"Coverage for amplitude\")\n", "\n", "ax[1].plot(p_nominals, contains_true[1].sum(0) / n_test)\n", "ax[1].plot([0, 1], [0, 1], color=\"black\", linestyle=\"--\")\n", "ax[1].set_xlabel(\"Nominal coverage\")\n", "ax[1].set_ylabel(\"Empirical coverage\")\n", "ax[1].set_title(\"Coverage for exponent\")"]}, {"cell_type": "markdown", "id": "112f7415", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["<a name='exercises_24_5'></a>     \n", "\n", "| [Top](#section_24_0) | [Restart Section](#section_24_5) | \n"]}, {"cell_type": "markdown", "id": "83095419", "metadata": {"tags": ["md", "learner", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 24.5.1</span>\n", "\n", "**Which one of the following answer options are acceptable calibration results for a posterior estimator?** In other words, which *likely* result would be preferable? Here, \"Overconfident\" means that one expects the true values to appear within a certain probability interval more often than is found using test samples. This was clearly the case for the amplitude in the example studied in the related video. In contrast, \"Conservative\" means the opposite, namely that the true value is found more often than expected in the given interval. This is the case for the exponent in the video, albeit at a much smaller level.\n", "\n", "A) Overconfident\\\n", "B) Conservative\\\n", "C) Perfectly calibrated\\\n", "D) Perfectly calibrated or conservative\\\n", "E) Perfectly calibrated or overconfident\n", "\n", "<br>"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}